\chapter{Background}

\ifpdf
    \graphicspath{{Chapter2/Figs/Raster/}{Chapter2/Figs/PDF/}{Chapter2/Figs/}}
\else
    \graphicspath{{Chapter2/Figs/Vector/}{Chapter2/Figs/}}
\fi

The goal of this chapter is to provide only the necessary background in
recurrent neural networks and generative probabilistic sequence modelling
required for understanding our models, experiments, and results. It also
introduces some common definitions and clarifies notation used throughout later chapters.

A basic understanding of Western music theory and neural networks is assumed.
Readers unfamiliar with concepts such as piano rolls, Roman numeral analysis,
and cadences, should review \vref{sec:music-theory-primer} for a quick primer
and \citet{piston1978harmony,denny1960oxford} for more thorough coverage.
Likewise, those whom wish to review concepts such as activation functions,
neurons, and applying recurrent neural networks over arbitrary length sequences
are advised to review \vref{sec:primer-nn} and consult
\citet{bengio2009learning} for further reference.

\section{Recurrent neural networks}\label{sec:bg-rnn}

Our use of the term \emph{recurrent neural network} (RNN) refers in particular
to linear Elman-type RNNs \citep{elman1990finding} whose dynamics are described
by \vref{eq:rnn-dynamics} (review \cref{sec:primer-nn} if this is unfamiliar).

\subsection{Notation}

\nomenclature[z-RNN]{RNN}{Recurrent Neural Network}
\nomenclature[a-input]{$\x$}{layer inputs}
\nomenclature[a-hidden-state]{$\h$}{hidden state (\ie memory cell contents)}
\nomenclature[a-output]{$\y$}{layer outputs}
\nomenclature[a-Wxh]{$\W$}{weight matrix}
\nomenclature[a-Nin]{$N_{in}$}{dimensionality of inputs}
\nomenclature[a-Nhid]{$N_{hid}$}{dimensionality of hidden state}
\nomenclature[a-Nout]{$N_{out}$}{dimensionality of outputs}
\nomenclature[a-T]{$T$}{total number of timesteps in a sequence}
\nomenclature[g-sxh]{$\sigma$}{elementwise activation function}
\nomenclature[g-th]{$\theta$}{model parameters}
\nomenclature[r-l]{$(l)$}{layer index in multi-layer networks}
\nomenclature[s-t]{$t$}{time index}
\nomenclature[s-st]{$st$}{connections from source $s$ to target $t$}

We begin by clarifying common notation and conventions used to describe RNNs.
Unless otherwise specified, future use of notation should be interpreted as
defined in this section.

We use the subscript $t \in \{ 1,2,\cdots,T \}$ to denote the \emph{time index}
within a sequence of length $T \in \NN$

A sequence of \emph{inputs} is denoted by $\x$ and the sequence elements at timestep $t$ is
denoted by $\x_t \in \RR^{N_{in}}$ and assumed to have dimensionality $N_{in} \in \NN$.
Similarly, $\h_t \in \RR^{N_{hid}}$ and $\y_t \in \RR^{N_{out}}$ denote elements
from the \emph{hidden state} and \emph{output} sequences respectively.

To describe model parameters, we use $\W$ to indicate a real-valued
\emph{weight matrix} consisting of all the connection weights between two sets
of neurons and $\sigma(\cdot)$ to indicate an elementwise \emph{activation
function}. The collection of all model parameters is denoted by $\vec{\theta}$.

When further clarity is required, we use subscripts $\W_{st}$ denote the
connection weights from a set of neurons $s$ to another set of neurons $t$ (\ie
in \vref{sec:LSTM}, $\W_{xf}$ and $\W_{xh}$ refer to the connections from the
inputs to the forget gate and hidden state respectively). Subscripts on
activation functions $\sigma_{s,t}(\cdot)$ are to be interpreted analogously.

Equipped with the above notation, the equations for RNN time dynamics can be
expressed as
\begin{equation}\label{eq:rnn-dynamics}
 \left.\begin{aligned}
          \h_t &=& \W_{xh} \sigma_{xh} \left( \x_t \right) + \W_{hh} \sigma_{hh} \left( \h_{t-1} \right)\\
          \y_t &=& \W_{hy} \sigma_{hy} \left( \h_t \right)
       \end{aligned}
 \right\}
 \qquad \text{RNN time dynamics}
\end{equation}

When discussing multi-layer networks, we use $L \in \NN$ to denote total number
of layers and parenthesized superscripts $(l)$ for $l \in \{1,2,\cdots,L\}$ to
indicate the layer. For example, $\z^{(2)}_t$ is the hidden states of the
second layer and $N^{(3)}_{in}$ is the dimensionality of the third layer's
inputs $\x^{(3)}_t$. Unless stated otherwise, multi-layer networks will assume
that the outputs of the $l-1$st layer are used as the inputs of the $l$th layer
(\ie $\forall t: \x^{(l)}_t = \y^{(l-1)}_t$).

\subsection{The memory cell abstraction}

While a large number of proposed RNN variants exist
\citep{elman1990finding,jordan1997serial,hochreiter1997long,cho2014learning,Koutnik2014,Mikolov2015},
most share the same underlying structure and differ only in their
implementation details of \cref{eq:rnn-dynamics}. Encapsulating these
differences within an abstraction enables general discussion about RNN
architecture without making a specific choice on implementation.

To do so, we introduce the \emph{memory cell} abstraction to encapsulate the
details of computing $\y_t$ and $\h_t$ from $\x_t$ and $\h_{t-1}$. This is
illustrated visually in \cref{fig:rnn-elman}, which shows a standard Elman-type
RNN \citep{elman1990finding} with the memory cell indicated by a dashed box
isolating the recurrent hidden state. The edges entering the memory cell
($\x_t$, $\h_{t-1}$) are the \emph{memory cell inputs} and the outgoing edges
($\y_t$, $\h_t$) are the \emph{memory cell outputs}. In essence, the memory
cell abstracts away differences across RNN variants in their implementation of
\cref{eq:rnn-dynamics}.

\begin{figure}[tb]
  \centering
  \input{Chapter2/Figs/nn-rnn-elman.pdf_tex}
  \caption{An Elman-type RNN with a single hidden layer. The recurrent hidden
    state is illustrated as unit-delayed (denoted by $z^{-1}$) feedback edges
    from the hidden states to the input layer. The memory cell encapsulating the
  hidden state is also shown.}
  \label{fig:rnn-elman}
\end{figure}

\subsection{Operations on RNNs: stacking and unrolling}

\subsubsection{Stacking memory cells to form deep RNNs}

Just like deep neural networks, RNNs can be \emph{stacked} to form deep RNNs
\citep{el1995hierarchical,schmidhuber1992learning} by treating the
outputs from the $l-1$st layer's memory cells as inputs to the $l$th layer (see \cref{fig:rnn-multi-unrolled}).

\begin{figure}[tb]
    \centering
    \resizebox{4.5in}{!}{\input{Chapter2/Figs/rnn-multi-unrolled.pdf_tex}}
    \caption{Block diagram representation of a -layer RNN (left) and its
    corresponding DAG (right) after unrolling. The blocks labelled
    with $\h_t$ represent memory cells whose parameters are shared across all times
  $t$.}
    \label{fig:rnn-multi-unrolled}
\end{figure}

Prior work has observed that ``deep RNNs outperformed the conventional, shallow RNN''
\citet{pascanu2013construct}, affirming the importance of stacking multiple layers
in RNNs. The improved modelling   can be attributed to two primary
factors: composition of multiple non-linear activation functions and an
increase in the number of paths for backpropagated error signals to flow. The
former reason is analogous to the case in deep belief networks, which is well
documented \citep{bengio2009learning}. To understand the latter, notice that in
\cref{fig:rnn-multi-unrolled} there is only a single path from $\x_{t-1}$ to
$\y_{t}$ hence the conditional independence $\y_{t} \independent \x_{t-1} |
\h^{(1)}_t$ is satisfied. However, in \cref{fig:rnn-multi-unrolled} there are
multiple paths from $\x_{t-1}$ to $\y_{t}$ (\eg passing through either
$\h^{(2)}_{t-1} \to \h^{(2)}_t$ or $\h^{(1)}_{t-1} \to \h^{(1)}_t$) through
which information may flow.

\subsubsection{Unrolling RNNs into directed acyclic graphs}

\nomenclature[z-DAG]{DAG}{Directed Acyclic Graph}

Given an input sequence $\{\x\}_{t=1}^T$, an RNN can be \emph{unrolled} into a
\emph{directed acyclic graph} (DAG) comprised of $T$ copies of the memory cell
connected forwards in time. This is illustrated for a stacked 2-layer RNN in
\cref{fig:rnn-multi-unrolled}, where the vectors $\y_t$, $\h_t$, and $\x_t$ are
depicted as blocks and the $\h_t$ is understood to represent a memory cell.

% \begin{figure}[tb]
%   \centering
%   \resizebox{4.5in}{!}{\input{Chapter2/Figs/rnn-single-unrolled.pdf_tex}}
%   \caption{Signal flow diagram representation of a single-layer RNN (left) and its
%     corresponding DAG (right) after unrolling. The blocks labelled
%     with $\h_t$ represent memory cells whose parameters are shared across all times
%   $t$.}
%   \label{fig:rnn-single-unrolled}
% \end{figure}

\Cref{fig:rnn-multi-unrolled} shows that the hidden state $\h_t$ is passed
forwards throughout the sequence of computations. This gives rise to an
alternative interpretation of the hidden state as a temporal memory mechanism.
Under this interpretation, updating the hidden state $\h_t$ can be viewed as
\emph{writing} information from the current inputs $\x_t$ to memory and
producing the outputs $\y_t$ can be interpreted as \emph{reading} information
from memory.

% Additionally, parameters need not be shared
% across different layers so the stacked RNN can learn different time dynamics
% for each layer.

\subsection{Training RNNs and backpropagation through time}

\nomenclature[z-BPTT]{BPTT}{Backpropagation Through Time}
\nomenclature[o-E]{$\mathcal{E}$}{error or loss}
\nomenclature[o-Et]{$\mathcal{E}_t$}{error or loss at time $t$}

The parameters $\vec{\theta}$ of a RNN are typically learned from data by
minimizing some \emph{cost} $\mathcal{E} = \sum_{1 \leq t \leq T}
\mathcal{E}_t(\x_t)$ measuring the performance of the network on some task.
This optimization is usually performed using iterative methods which require
computation of gradients $\frac{\pd \mathcal{E}}{\pd \vec{\theta}}$ at each
iteration.

In feed-forward networks, computation of gradients can be performed efficiently
using backpropagation
\citep{bryson1963optimal,linnainmaa1970representation,rumelhart1988learning}.
While time-delayed recurrent hidden state connections appear to complicate
matters initially, unrolling the RNN removes the time-delayed recurrent edges
and converts the RNN into a DAG (\eg \vref{fig:rnn-multi-unrolled}) which can
be interpreted as a $T$ layered feed-forward neural network with parameters
shared across all $T$ layers.

This view of unrolled RNNs as feedforward networks motivates
\emph{backpropagation through time} (BPTT) \citep{goller1996learning}, a method
for training RNNs which applies backpropagation to the unrolled DAG.

\begin{figure}[tb]
    \centering
    \input{Chapter2/Figs/rnn-bptt.pdf_tex}
    \caption{The gradients accumulated along network edges in BPTT.}
    \label{fig:rnn-bptt}
\end{figure}

\Cref{fig:rnn-bptt} shows how BPTT, just like regular backpropagation, divides
the computation of a global gradient $\frac{\pd \mathcal{E}}{\pd \theta}$ into
a series of local gradient computations, each of which involves significantly
less variables and is hence cheaper to compute. However, whereas the depth of
feedforward networks is fixed, the unrolled RNN's depth is equal to the input
sequence length $T$ and may introduce problems when $T$ is very large.

\subsubsection{Vanishing/exploding gradients}

It is well known that naive implementations of memory cells often suffer from
two problems also affecting very deep feedforward networks: the \emph{vanishing
gradient} and \emph{exploding gradient} \citep{Bengio1994}.

To illustrate the problem, express the computation represented by
\cref{fig:rnn-bptt} mathematically by applying the chain rule to
the RNN dynamics equation (\vref{eq:rnn-dynamics}):
\begin{align}
  \frac{\pd \mathcal{E}}{\pd \vec{\theta}} &= \sum_{1 \leq t \leq T} \frac{\pd \mathcal{E}_t}{\pd \vec{\theta}} \label{eq:err-total}\\
    \frac{\pd \mathcal{E}_t}{\pd \vec{\theta}} &= \sum_{1 \leq k \leq t} \left(
        \frac{\pd \mathcal{E}_t}{\pd \y_t}
        \frac{\pd \y_t}{\pd \h_t}
        \frac{\pd \h_t}{\pd \h_k}
        \frac{\pd \h_k}{\pd \vec{\theta}}
    \right) \label{eq:error-t}\\
    \frac{\pd \h_t}{\pd \h_k} &=
    \prod_{t \geq i > k} \frac{\pd \h_i}{\pd \h_{i-1}}
    = \prod_{t \geq i > k} \W_{hh}^\tp \diag \left( \sigma_{hh}'( \h_{i-1} ) \right)
    \label{eq:error-transfer}
\end{align}

\Cref{eq:error-t} expresses how the error $\mathcal{E}_t$ at time $t$ is a sum
of \emph{temporal contributions} $
\frac{\pd \mathcal{E}_t}{\pd \y_t}
\frac{\pd \y_t}{\pd \h_t}
\frac{\pd \h_t}{\pd \h_k}
\frac{\pd \h_k}{\pd \vec{\theta}}$
measuring how $\vec{\theta}$'s impact on $\h_k$ affects the cost
$\mathcal{E}_t$ at some future time $t > k$. The quantity
$\frac{\pd \h_t}{\pd \h_k}$ in \cref{eq:error-transfer} measures the affect of
the hidden state $\h_k$ on some future state $\h_t$ where $t > k$ and can be
interpreted as transferring the error ``in time'' from step $t$ back to step
$k$ \citep{Pascanu2012}.

Both vanishing and exploding gradients are due to the product in
\cref{eq:error-transfer} exponentially growing or shrinking over long
time-spans (\ie $t \gg k$), preventing error signals to be transferred across
long time-spans and learning of long-term dependencies. In
\vref{sec:vanishing-exploding-gradients} we prove that a sufficient condition
for vanishing gradients is:
\begin{equation}\label{eq:vanishing-gradients-suff}
  \left\| \W_{hh} \right\| < \frac{1}{\gamma_\sigma}
\end{equation}
where $\| \cdot \|$ is the matrix operator norm (see \vref{eq:operator-norm}),
$\W_{hh}$ is as defined in \vref{eq:rnn-dynamics},
and $\gamma_\sigma$ is a constant depending on the choice of activation function
(\eg $\gamma_\sigma = 1$ for $\sigma_{hh} = \tanh$, $\gamma_\sigma = 0.25$ for
$\sigma_{hh} = \sigmoid$).

This difficulty learning relationships between events spaced far apart in time
presents a significant challenge for music applications. As noted by
\citet{cooper1963rhythmic}:
\begin{quote}
  Long-term dependencies are at the heart of what defines a style of music, with
  events spanning several notes or bars contributing to the formation of metrical and phrasal
  structure.
\end{quote}

\subsection{Long short term memory: solving the vanishing gradient}\label{sec:LSTM}

\nomenclature[z-LSTM]{LSTM}{Long Short Term Memory}
\nomenclature[z-CEC]{CEC}{Constant Error Carousel}
\nomenclature[a-input-gate]{$\i_t$}{input gate values at time $t$}
\nomenclature[a-forget-gate]{$\f_t$}{forget gate values at time $t$}
\nomenclature[a-output-gate]{$\o_t$}{output gate values at time $t$}
\nomenclature[x-odot]{$\odot$}{elementwise multiplication}

In order to build a model which learns long range dependencies, vanishing
gradients must be avoided. A popular memory cell architecture which does so is
\emph{long short term memory} (LSTM). Proposed by \citet{hochreiter1997long},
LSTM solves the vanishing gradient problem by enforcing \emph{constant error
flow} on \cref{eq:error-transfer}, that is
\begin{equation}\label{eq:const-err-flow}
    \forall t, \forall \h_t: \W_{hh}^\tp \sigma_{hh}' (\h_{t}) = \matr{I}
\end{equation}
where $\matr{I}$ is the identity matrix.

As a consequence of constant error flow, \vref{eq:error-transfer} becomes
\begin{equation}
  \frac{\pd \h_t}{\pd \h_k}
  = \prod_{t \geq i > k} \W_{hh}^\tp \diag \left( \sigma_{hh}'( \h_{i-1} ) \right)
  = \prod_{t \geq i > k} \matr{I}
  = \matr{I}
\end{equation}
The dependence on the time-interval $t-k$ is no longer present, ameliorating
the exponential decay causing vanishing gradients and enabling long-range
dependencies (\ie $t \gg k$) to be learned.

Integrating \cref{eq:const-err-flow} with respect to $\h_t$ yields $\W_{hh}
\sigma_{hh}(\h_{t}) = \h_{t}$. Since this must hold for any hidden state
$\h_{t}$, this means that:
\begin{enumerate}
    \item $\W_{hh}$ must be full rank
    \item $\sigma_{hh}$ must be linear
    \item $\W_{hh} \sigma_{hh} = \matr{I}$
\end{enumerate}

In the \emph{constant error carousel} (CEC), this is ensured by setting
$\sigma_{hh} = \W_{hh} = \I$. This may be interpreted as removing time dynamics
on $\h$ in order to permit error signals to be transferred backwards in time
(\cref{eq:error-transfer}) without modification (\ie $\forall t \geq k: \frac{\pd
\h_t}{\pd \h_k} = \I$).

In addition to using a CEC, a LSTM introduces three gates controlling access to the CEC:
\begin{description}
  \item[Input gate]: scales input $\x_t$ elementwise by $\i_t \in [0,1]$, \emph{writes} to $\h_t$
  \item[Output gate]: scales output $\y_t$ elementwise by $\o_t \in [0,1]$, \emph{reads} from $\h_t$
  \item[Forget gate]: scales previous cell value $\h_{t-1}$ by $\f_t \in [0,1]$, \emph{resets} $\h_t$
\end{description}

Mathematically, the LSTM model is defined by the following set of equations:
\begin{align}
    \i_t &= \sigmoid(\W_{xi} \x_t + \W_{yi} \y_{t-1} + \b_i) \\
    \o_t &= \sigmoid(\W_{xo} \x_t + \W_{yo} \y_{t-1} + \b_o) \\
    \f_t &= \sigmoid(\W_{xf} \x_t + \W_{yf} \y_{t-1} + \b_f) \\
    \h_t &= \f_t \odot \h_{t-1} + \i_t \odot \tanh(\W_{xh}\x_t + y_{t-1} \W_{yh} + \b_h) \\
    \y_t &= \o_t \odot \tanh(\h_t)
\end{align}
where $\odot$ denotes elementwise multiplication of vectors.

Notice that the gates ($\i_t$, $\o_t$, and $\f_t$) controlling flow in and out
of the CEC are time dependent. This permits interpreting the gates as a
mechanism enabling LSTM to learn which error signals to trap in the CEC and
when to release them \citep{hochreiter1997long}, allowing error signals to
potentially be transported across long time lags.

\begin{figure}[tb]
    \centering
    \input{Chapter2/Figs/lstm-unit-2.pdf_tex}
    \caption{Schematic for a single LSTM memory cell. Notice how the gates $\i_t$, $\o_t$, and $\f_t$ control access to the constant error carousel (CEC).}
    \label{fig:lstm-cell}
\end{figure}

Some authors define LSTM such that $\h_t$ is not used to compute gate
activations, referring to \cref{fig:lstm-cell} as LSTM with ``peephole
connections'' \citep{gers2000recurrent}. We will use LSTM to refer to the
model as described above.

\subsubsection{Practicalities for successful applications of LSTM}

Many successful applications of LSTM
\citep{devlin2014fast,zaremba2015empirical,pascanu2013construct} employ some
common practical techniques. Perhaps most important is \emph{gradient norm
clipping} \citep{Mikolov2012,Pascanu2012} where the gradient is scaled or
clipped whenever it exceeds a threshold. This is necessary because while
vanishing gradients are mitigated by CECs, LSTM do not explicitly protect
against exploding gradients.

Another common practice is the use of methods for reducing overfitting and
improving generalization. In particular, \emph{dropout}
\citep{hinton2012improving} is commonly applied between stacked memory cell
layers to regularize the learned features and prevent co-adaptation
\citep{zaremba2014recurrent}. Additionally, \emph{batch normalization}
\citep{ioffe2015batch} of memory cell hidden states is also commonly done to
reduce co-variate shifts, accelerate training, and improve generalization.

Finally, applications of RNNs to long sequences can incur a prohibitively high
cost for a single parameter update \citep{citeulike:13881859}. For instance,
computing the gradient of an RNN on a sequence of length $1000$ costs the
equivalent of a forward and backward pass on a $1000$ layer feed-forward
network. This issue is typically addressed by only back-propagating error
signals a fixed number of timesteps back in the unrolled network, a technique
known as \emph{truncated BPTT} \citep{williams1990efficient}. As the hidden
states in the unrolled network have already been exposed to many previous
timesteps, learning of long range structure is still possible with truncated
BPTT.

