\chapter{Background}

\ifpdf
    \graphicspath{{Chapter2/Figs/Raster/}{Chapter2/Figs/PDF/}{Chapter2/Figs/}}
\else
    \graphicspath{{Chapter2/Figs/Vector/}{Chapter2/Figs/}}
\fi

\mynote{Try mimicking \citet{franklin2006recurrent}}.

In this chapter, we provide background information on two topics heavily
utilized throughout this dissertation: Western music theory and recurrent
neural networks. In addition, we introduce definitions and notation used
throughout later chapters.

\section{A primer on Western music theory}

This section provides a primer on the music theory relevant for understanding
the following chapters. It synthesizes material originally presented in
\citet{franklin2006recurrent,nagler2014schubot,quick2014kulitta,freedman2015correlational}.
Readers with a strong music background may wish to skip to
\vref{sec:automatic-composition}.

Music theory is a branch of musicology concerned with the study of the rules
and practices of music. While the general field includes study of acoustic
qualities such as dynamics and timbre, we restrict the scope of our research to
modeling musical \emph{scores} (\eg \cref{fig:eg-score}) and neglect issues
related to articulation and performance (\eg dynamics, accents, changes in
tempo) as well as synthesis/generation of the physical acoustic waveforms.

\begin{figure}[tb]
    \centering
    \includegraphics[trim={0 23cm 0 0},clip,width=1.0\linewidth]{bwv133-6-eg-score.pdf}
    \caption{Sheet music representation of the first four bars of BWV 133.6}
    \label{fig:eg-score}
\end{figure}

This is justified because the physical waveforms are more closely related
to the skill of the performers and instruments used and are likely to vary
significantly across different performances. Furthermore, articulations in
the same musical piece may differ across transcriptions and performers.
Despite these variations, a piece of music is still recognizable from just the
notes, suggesting that notes are the defining element for a piece of music.

\subsection{Notes: the basic building blocks}

A \emph{note} is the most fundamental element of music score and
represents a sound played at a certain \emph{pitch} for a certain
\emph{duration}. In sheet music such as \cref{fig:eg-score}, the notes are
denoted by the filled/unfilled black heads with protruding stems. As a can be
viewed as a collection of notes over time, notes are the fundamental building
blocks for musical scores.

\subsubsection{Pitch}

Though pitch is closely related to physical frequency of vibration of a
waveform (as measured in Hertz), pitch is a perceptual property whose semantic
meaning is derived from a listener's perception. This distinction has been
scrutinized by \citet{:/content/asa/journal/jasa/55/5/10.1121/1.1914648}, whose
visual analogy in \cref{fig:pitch} illustrates how a pitch can be heard even if
its percieved frequency is absent just as one may see the word ``PITCH''
despite being presented with only a suggestive shadow.

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.6\linewidth]{pitch.pdf}
    \caption{Terhardt's visual analogy for pitch. Similar to how the viewer of this figure may percieve contours not present, pitch describes subjective information received by the listener even when physical frequencies are absent.}
    \label{fig:pitch}
\end{figure}

Despite its psychoacoustic nature, it is nevertheless useful to objectively
quantify pitch as a frequency. To do so, we first need some definitions. The
difference between two frequencies is called an \emph{interval} and an
\emph{octave} is an interval corresponding to the distance between a frequency
$f \in \RR^+$ and its doubling $2f$ or halving $f/2$. Two frequencies spaced
exactly an octave apart are perceived to be similar, suggesting that music is
percieved on a logarithmic scale.

Most Western music is based on the \emph{twelve-note chromatic scale}, which
divides an \emph{octave} into twelve distinct frequencies. The \emph{tuning
system} employed dictates the precise intervals between subdivisions, with
\emph{equal temperament tuning} (all subdivisions are equally spaced on a
logarithmic scale) the most widely used in common practice music
\citep{denton1997history}. Under twelve-note equal temperament tuning, the
distance between two consecutive subdivisions ($1/12$ of an octave) is called a
\emph{semitone} (British) or \emph{half-step} (North American) and two
semitones constitutes a \emph{tone} or \emph{whole-step}.

When discussing music, \emph{note names} which enable succinct specification of
a musical pitch are often employed. In \emph{scientific pitch notation},
\emph{pitch classes} which represent a pitch modulo the octave are specified by
a letter ranging from $A$ to $G$ and optionally a single \emph{accidental}. Pitch
classes without accidentals are called \emph{natural} and correspond to the white
keys on a piano. Two accidentals are possible: sharps ($\#$) raise the natural
pitch class up one semitone and flats ($\flat$) lower by one semitone.
\cref{fig:piano-keys} illustrates how these pitch classes map to keys on a
piano.

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.6\linewidth]{piano-keys.pdf}
    \caption{Illustration of an octave in the 12-note chromatic scale on a piano keyboard.}
    \label{fig:piano-keys}
\end{figure}

Since pitch classes represent equivalence class of frequencies spaced an
integral number of octaves apart, unambiguously specifying a pitch requires not
only a pitch class but also an octave. In scientific pitch notation, this is
accomplished by appending an octave number to a pitch class letter (see
\cref{fig:pitch-class}). Together, a pitch class and octave number uniquely
specify the notation for a pitch. On sheet music, the pitch of a note is
indicated by its vertical position with respect to the \emph{stave} (the five
horizontal lines and four spaces).

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.6\linewidth]{Pitch_notation.png}
    \caption{Scientific pitch notation and sheet music notation of $C$ notes at ten different octaves.}
    \label{fig:pitch-class}
\end{figure}

\subsubsection{Duration}

In addition to pitch, a note also possesses a \emph{duration}. The duration of
a note indicates how long it is to be played and is measured in fractions of a
\emph{whole note} (American) or \emph{semibreve} (British). Perhaps the most
common duration is a \emph{quarter-note} (American) or \emph{crotchet}
(British). Other note durations are also possible and the most common along
with their notation in sheet music are enumerated in
\cref{fig:note-durations}. The relationship between durations and
physical time intervals is given by the \emph{tempo}, which is usually
denoted near the start of the piece in beats per minute.

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.6\linewidth]{note-durations.png}
    \caption{Comparison of various note durations \citep{wiki-note-durations}}
    \label{fig:note-durations}
\end{figure}

\subsubsection{Offset, Measures, and Meter}

The final property a note possess is an \emph{offset} indicating the time at
which a note is articulated. The offset is measured with respect to a fixed
reference point in time, such as the start of a score.

Another common reference point for measuring offsets is with respect to the
preceding \emph{bar}. Bars are denoted by vertical lines through the
stave in sheet music and are used to subdivide a piece into smaller temporal
units called \emph{measures}. Except for the notes preceeding the first bar
(\ie the \emph{anacrusis}), most measures within a score all have the same
duration.

In \vref{fig:eg-score}, the crotchet preceding the first bar provides an
example of an anacrusis. Notice that all other measures in the score are four
crotchets in duration. In addition, observe that the offsets of notes within a
measure is highly repetitive. There is always a note articulated on the first
crotchet of a measure and articulations occuring between crotchets (\ie
quavers) are only present the last two crotchets of a measure.

This repetition of the same pattern of offsets across multiple measure helps
establish a periodic pattern of strong and weak beats, a concept known as
\emph{meter} \cite{grove-meter}. Meter is implied in Western music, where bars
establish perodic measures of equal length \citep{handel1993listening}. The
meter of a score provides information about musical structure which can be used
to predict chord changes and repepetition boundaries
\citep{cooper1963rhythmic}.

\subsubsection{Piano roll notation}

\begin{figure}[tb]
    \centering
    \input{Chapter2/Figs/bwv133-6-eg-piano.pgf}
    \caption{Piano roll notation of the music in \cref{fig:eg-score}}
    \label{fig:eg-piano-roll}
\end{figure}

\Cref{fig:eg-piano-roll} shows the the same score from
\vref{fig:eg-score} in \emph{piano roll notation}, a format which is convenient
for visualization purposes. The horizontal and vertical axes represent time and
pitch respectively. A solid horizontal bar signifies the presence of a note at
the corresponding pitch and offset and the length of the bar corresponds to the
note's duration.

\subsection{Tonality in common practice music}

\emph{Tonality} refers to ``the orientation of melodies and harmonies towards a
referential (or \emph{tonic}) pitch class`` \citep{grove-tonality}. One way to
characterize tonality is with \emph{scales}, which defines a subset of pitch
classes that are ``in key'' with respect to the tonic. \Cref{tab:key-intervals}
shows the pitch intervals between adjacent pitch classes within two important
scales: the \emph{major} and the \emph{minor}. The choice of tonic and scale is
collectively referred to as the \emph{key}.

\begin{table}[tb]
    \centering
    \caption{Pitch intervals for the two most important keys \citep{freedman2015correlational}. The pitches in a scale can be found by starting at the tonic and successively offsetting by the given pitch intervals.}
    \label{tab:key-intervals}
    \begin{tabular}{lc}
        \toprule
        Key & Pitch Intervals (semitones) \\
        \midrule
        Major (Ionian, I) & +2, +2, +1, +2, +2, +2 \\
        Minor (Aeolian, VI) & +2, +1, +2, +2, +1, +2 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Polyphony, chords, and chord progressions}

Whereas \emph{monophonic} music is characterized by the presence of a single
\emph{part} sounding at most one note at any given time, \emph{polyphonic}
music contains multiple parts potentially sounding multiple pitches at the same
time. Just as notes form the basis of monophonic music, chords are the fundamental
building blocks for polyphonic music.

\subsubsection{Chords: basic units for representing simultaneously sounding notes}

A \emph{chord} is a collection of three or more pitches all sounding
simultaneously \citep{randel1999harvard}. In Western classical music, they
typically consist of a \emph{root note} whose pitch class forms a base from
which successive notes are built upon. The intervals between the pitch classes
in a chord are commonly labeled using \emph{qualities}, which are invariant
across octaves. Different realizations of the same chord (\eg octave choices
for each pitch class) are called \emph{voicings}.

\cref{tab:chord-qualities} lists some common chord qualities and their
corresponding intervals from the root note. Chord names are given as a root
pitch class followed by a quality, for example: $C$ major, $A$ minor, or $G$
half-diminished $7$th.

\begin{table}[tb]
    \centering
    \caption{Common chord qualities and their corresponding intervals \citep{freedman2015correlational}}
    \label{tab:chord-qualities}
    \begin{tabular}{lc}
        \toprule
        Chord quality & Intervals from root pitch class \\
        \midrule
        Major & +4, +7 \\
        Major 6th & +4, +7, +8 \\
        Major 7th & +4, +7, +11 \\
        Minor & +3, +7 \\
        Minor 6th & +3, +7, +9 \\
        Minor 7th & +3, +7, +10 \\
        Dominant 7th & +4, +7, +10 \\
        Augmented & +4, +8 \\
        Diminished & +3, +6 \\
        Diminished 7th & +3, +6, +9 \\
        Half-diminished 7th & +3, +6, +10 \\
        \bottomrule
    \end{tabular}
\end{table}

The lowest note in a chord is called the \emph{bass} note and is oftentimes the
root note. However, alternative voicings called \emph{inversions} can place the
root note on a different octave and cause the bass and root notes to
differ.

\subsubsection{Chord progressions, phrases, and cadences}

Sequences of chords are called \emph{chord progressions}, which are oftentimes
grouped with adjacent progressions into coherent units called \emph{phrases}.
Many psychoacoustic phenomena such as stability, mood, and expectation can be
attributed choice of chord progressions and phrase structure. For example,
chord progressions can be used to create \emph{modulations} which transition
the music into a different key.

Analyzing chord progresssions involves a degree of subjectivity as chords can
be overlapping and contain extraneous notes or involve uncommon chord
qualities. A common method for analyzing chord progressions is \emph{Roman
numeral analysis}, where \RN{1} is used for denoting the tonic pitch class,
successive Roman numerals for successive pitch classes in the key, and
capitalization is used to distinguish major and minor qualities. For example,
the chord progression $C$ major -- $A$ minor -- $D$ major 7th -- $G$ major in
the $C$ major key would be represented in Roman numerals as \RN{1} -- \Rn{2} --
\RN{2}maj$7$ -- \RN{5}.

A common use case for Roman numeral analysis is identifying and classifying
chord progressions called \emph{harmonic cadences}, which are commonly used for
effects such as eliciting a sense of incompleteness
\citep{jonas1982introduction} or establishing a sense of conclusion at the end
of phrases \citep{randel1999harvard}. The most important cadences include:
\mynote{Review that these definitions are actually necessary}
\begin{description}
  \item[Perfect cadence]: \RN{5} -- \RN{1}. The perfect cadence is described by
    \citet{randel1999harvard} as ``a microcosm of the tonal system, and is the
    most direct means of establishing a pitch as tonic. It is virtually
    obligatory as the final structural cadence of a tonal work''
  \item[Imperfect cadence]: Any cadence ending on \RN{5}, including
    \RN{1}--\RN{5}, \Rn{2}--\RN{5}, \RN{4}--\RN{5}, \RN{5}--\RN{5}, and \Rn{6}
    -- \RN{5}. The imperfect cadence sounds incomplete and is considered
    a \emph{weak} cadence which call for continuation \citep{jonas1982introduction}
  \item[Plagal cadence]: \RN{4}--\RN{1}
  \item[Interrupted cadepnce]: \RN{5}--\Rn{6}. Also considered a weak cadence
    which invokes a ``hanging'' sensation prompting continuation
    \cite{piston1978harmony}.
\end{description}

\subsubsection{Transposition invariance}\label{sec:transposition-invariance}

Notice that the discussion thus far has remained ambiguous on the choice of
tonic. This is intentional: most of the concepts discussed do not depend on the
choice of tonic. When discussing tonality, the scale was defined using
intervals relative to a choice of tonic. Similarly, Roman numeral analysis of
chord progressions and cadences is also conducted relative to a tonic. Neither
the scale nor the Roman numeral analysis is affected when a score is transposed
by an arbitrary pitch interval.

The \emph{transposition invariance} of chord progressions and keys is an
important property of music. It enables us to offset an entire score by an
arbitrary pitch interval without affecting many important psychoacoustic
qualities.

\section{Automatic composition}\label{sec:automatic-composition}

\emph{Automatic composition} broadly refers to the use of algorithms to
generate music. \citet{papadopoulos1999ai} gives the following definition for
automatic composition:
\begin{quote}
  a sequence (set) of rules (instructions, operations) for solving
  (accomplishing) a [particular] problem (task) [in a finite number of steps]
  of combining musical parts (things, elements) into a whole (composition)
\end{quote}
Of the many tasks within automatic composition, two are most relevant to us:
\begin{description}
  \item[Harmonization] Given a fixed part (\eg a melody), generate the notes for
    one or more accompanying parts
  \item[Composition] Generate all the parts for a score of music
\end{description}
In addition, our work primarily on \emph{automatic stylistic composition} where
we impose the additional constraint that the generated music should be
stylistically similar to a particular genre or artist.

\section{Recurrent neural networks for generative sequence modelling}

\nomenclature[z-RNN]{RNN}{Recurrent Neural Network}

Our work in later sections make heavy use of recurrent neural networks (RNNs).
In this section, we clarify terminology and review the relevant ideas.
While we assume a basic understanding of neural networks, readers lacking
such a background may be interested in first reviewing \vref{sec:primer-nn}
before proceeding.

\subsection{Notation}

\nomenclature[r-input]{$\x^{(l)}_t$}{Input to layer $l$ at time $t$}
\nomenclature[r-hidden-state]{$\h^{(l)}_t$}{Hidden state of layer $l$ at time $t$}
\nomenclature[r-output]{$\y^{(l)}_t$}{Output of layer $l$ at time $t$}
\nomenclature[r-Wxh]{$\W_{xh}$}{Weight matrix between input and hidden state}
\nomenclature[r-Wxh]{$\W_{hh}$}{Weight matrix between previous and current and hidden states}
\nomenclature[r-Wxh]{$\W_{hy}$}{Weight matrix between hidden state and output}
\nomenclature[g-sxh]{$\sigma_{xh}$}{Activation function between input and hidden state}
\nomenclature[g-shh]{$\sigma_{hh}$}{Activation function between previous and current hidden states}
\nomenclature[g-shy]{$\sigma_{hy}$}{Activation function between hidden state and output}

We first clarify the basic notation and conventions used to describe multilayer
RNNs. Unless otherwise specified, unintroduced notation appearing in the
remainder of our work is to be interpreted as defined in this section.

We use $\x_t \in \RR^{N_{in}}$ with $t=1,2,\cdots,T$ to denote a sequence of
\emph{input} (\ie observed) vectors, $\z_t \in \RR^{N_{hid}}$ a sequence of
\emph{hidden state} (\ie unobserved) vectors. and $\y_t \in \RR^{N_{out}}$ a
sequence of \emph{output} vectors.

To describe model parameters, we use $\W$ to indicate the \emph{weight matrix}
consisting of all the connection weights between two blocks of neurons
and $\sigma(\cdot)$ to indicate the \emph{activation function}. The collection
of all model parameters is denoted by $\vec{\theta}$.

When further clarity is required, we use $\W_{s,t}$ to denote the connection
weights from block $s$ to block $t$ (\ie in \cref{sec:LSTM}, $\W_{xf}$ and
$\W_{xh}$ refer to the connections from the inputs to the forget gate and
hidden state respectively). Subscripts on activation functions $\sigma_{s,t}(\cdot)$
are to be interpreted analogously.

Using the above notation, the equations for \emph{RNN time dynamics} can be expressed as
\begin{equation}\label{eq:rnn-dynamics}
 \left.\begin{aligned}
          \h_t &=& \W_{xh} \sigma_{xh} \left( \x_t \right) + \W_{hh} \sigma_{hh} \left( \h_{t-1} \right)\\
          \y_t &=& \W_{hy} \sigma_{hy} \left( \h_t \right)
       \end{aligned}
 \right\}
 \qquad \text{RNN time dynamics}
\end{equation}

When discussing multilayer networks, we use parenthesized superscripts to
indicate layer. For example, $\z^{(2)}_t$ is the hidden states of the second
layer and $N^{(3)}_{in}$ is the dimensionality of the third layer's inputs
$\x^{(3)}_t$. We assume the outputs of the $l-1$st layer are used as the inputs
of the $l$th layer (\ie $\forall t: \x^{(l)}_t = \y^{(l-1)}_t$).

\subsection{The memory cell abstraction}

While many different variants for RNNs exist
\citep{elman1990finding,jordan1997serial,hochreiter1997long,cho2014learning,Koutnik2014,Mikolov2015},
many share the same underlying structure. Hence, it is useful to discuss RNNs
abstractly without specifying a particular variant.

\begin{figure}[tb]
  \centering
  \input{Chapter2/Figs/nn-rnn-elman.pdf_tex}
  \caption{An Elman-type RNN with a single hidden layer. The recurrent hidden
    state is illustrated as unit-delayed (denoted by $z^{-1}$) feedback edges
    from the hidden states to the input layer. The memory cell encapsulating the
  hidden state is also shown.}
  \label{fig:rnn-elman}
\end{figure}

To do so, we introduce the notion of a \emph{memory cell} in order to abstract
away how different variations of RNNs compute $\y_t$ and $\h_t$ from $\x_t$ and
$\h_{t-1}$. This is illustrated visually in \cref{fig:rnn-elman}, which shows a
standard Elman-type RNN \citep{elman1990finding} with the memory cell indicated
as a dashed box isolating the recurrent hidden state.

Notice that the edges in \cref{fig:rnn-elman} entering the memory cell
consist of the input $\x_t$ and previous hidden state $\h_{t-1}$,
and the edges leaving the memory cell consist of the current hidden state $\h_t$
and the outputs $\y_t$. Hence, to specify an concrete implementation for a
memory cell, it suffices to provide two functions $\f_h$ and $\f_y$ which
uses $\x_t$ and $\h_{t-1}$ to compute the next hidden state $\h_t = f_h(\x_t,
\h_{t-1})$ and output $\y_t = f_y(\h_t)$.

\subsection{Operations on RNNs: unrolling and stacking}

\subsubsection{Unrolling RNNs into directed acyclic graphs}

\nomenclature[z-DAG]{DAG}{Directed Acyclic Graph}

Given an input sequence $\{\x_{t}\}_{t=1}^T$ with length $T$, an RNN can be
\emph{unrolled} into a directed acyclic graph (DAG) comprised of $T$ copies of the
memory cell connected forwards in time. \Cref{fig:rnn-single-unrolled} shows
a block diagram of \cref{fig:rnn-elman} on the left and its corresponding unrolled
DAG on the right.

\begin{figure}[tb]
  \centering
  \resizebox{4.5in}{!}{\input{Chapter2/Figs/rnn-single-unrolled.pdf_tex}}
  \caption{Signal flow diagram representation of a single-layer RNN (left) and its
    corresponding DAG (right) after unrolling. The blocks labeled
    with $\h_t$ represent memory cells whose parameters are shared across all times
  $t$.}
  \label{fig:rnn-single-unrolled}
\end{figure}

\Cref{fig:rnn-single-unrolled} shows that the hidden state $\h_t$ is passed
forwards throughout the sequence of computations. This gives rise to an
alternative interpretation of the hidden state as a temporal memory mechanism.
Under this interpretation, updating the hidden state $\h_t = f_h (\x_t,
\h_{t-1})$ can be viewed as \emph{writing} information from the current inputs
$\x_t$ to memory and producing the outputs $\y_t = f_y (\h_t)$ can be
interpreted as \emph{reading} information from memory.

\subsubsection{Stacking memory cells to form deep RNNs}

In addition to unrolling, RNNs can also be \emph{stacked} to form deep RNNs
\citep{el1995hierarchical,schmidhuber1992learning}. This is accomplished in a
manner analogous to deep belief networks: outputs from the previous memory cell
in the stack are used as inputs for the current memory cell (see
\cref{fig:rnn-multi-unrolled}).

\begin{figure}[tb]
    \centering
    \resizebox{4.5in}{!}{\input{Chapter2/Figs/rnn-multi-unrolled.pdf_tex}}
    \caption{Stacking multiple memory cells to form a deep RNN.}
    \label{fig:rnn-multi-unrolled}
\end{figure}

The greater modeling capabilities of multilayer RNNs can be attributed to three
primary factors: composition of multiple non-activation functions and an
increase in the number of paths for information can flow.
The former reason is analogous to the case in deep belief networks, which
is well documented \citep{bengio2009learning}. To understand the latter, notice
that in \cref{fig:rnn-single-unrolled} there is only a single path from
$\x_{t-1}$ to $\y_{t}$ hence the conditional independence $\y_{t} \independent
\x_{t-1} | \h^{(1)}_t$ is satisfied. However, in \cref{fig:rnn-multi-unrolled}
there are multiple paths from $\x_{t-1}$ to $\y_{t}$ (\eg passing through
either $\h^{(2)}_{t-1} \to \h^{(2)}_t$ or $\h^{(1)}_{t-1} \to \h^{(1)}_t$)
through which information may flow.

% Additionally, parameters need not be shared
% across different layers so the stacked RNN can learn different time dynamics
% for each layer.

\subsection{Training RNNs and backpropagation through time}

\nomenclature[z-BPTT]{BPTT}{Backpropagation Through Time}
\nomenclature[g-th]{$\theta$}{Model Parameters}
\nomenclature[r-E]{$\mathcal{E}$}{Error or Loss}

The parameters $\vec{\theta}$ of a RNN are typically learned from data to
minimize some \emph{cost} $\mathcal{E} = \sum_{1 \leq t \leq T} \mathcal{E}_t(\x_t)$
measuring the performance of the network on some task. This optimization is
commonly carried out using iterative gradient descent methods, which require
computation of the gradients $\frac{\pd \mathcal{E}}{\pd \vec{\theta}}$ at each
iteration.

In feedforward networks, computation of gradients can be performed efficiently
using backprapogation
\citep{bryson1963optimal,linnainmaa1970representation,rumelhart1988learning}.
While the cycles introduced by time-delayed recurrent hidden state connections
may seem to complicate matters for RNNs, recall that unrolling removes the
time-delayed recurrent edges and converts the RNN into a DAG (\eg
\vref{fig:rnn-single-unrolled}). The unrolled RNN can be justifiably
interpreted as a $T$ layered feedforward neural network with parameters shared
across all layers, motivating the application of techniques such as
backpropagation to the unrolled RNNs.

Unsurprisingly, this is precisely what is done in the \emph{backpropagation
through time} (BPTT) algorithm \citep{goller1996learning}. Applying the
chain rule to the RNN dynamics equations (\vref{eq:rnn-dynamics})
unrolled network (see \cref{fig:rnn-bptt}), we obtain
\begin{align}
  \frac{\pd \mathcal{E}}{\pd \vec{\theta}} &= \sum_{1 \leq t \leq T} \frac{\pd \mathcal{E}_t}{\pd \vec{\theta}} \label{eq:err-total}\\
    \frac{\pd \mathcal{E}_t}{\pd \vec{\theta}} &= \sum_{1 \leq k \leq t} \left(
        \frac{\pd \mathcal{E}_t}{\pd \y_t}
        \frac{\pd \y_t}{\pd \h_t}
        \frac{\pd \h_t}{\pd \h_k}
        \frac{\pd \h_k}{\pd \vec{\theta}}
    \right) \label{eq:error-t}\\
    \frac{\pd \h_t}{\pd \h_k} &=
    \prod_{t \geq i > k} \frac{\pd \h_i}{\pd \h_{i-1}}
    = \prod_{t \geq i > k} \W_{hh}^\tp \diag \left( \sigma_{hh}'( \h_{i-1} ) \right)
    \label{eq:error-transfer}
\end{align}

\Cref{eq:error-t} expresses how the error $\mathcal{E}_t$ at time $t$ is a sum
of \emph{temporal contributions} $
\frac{\pd \mathcal{E}_t}{\pd \y_t}
\frac{\pd \y_t}{\pd \h_t}
\frac{\pd \h_t}{\pd \h_k}
\frac{\pd \h_k}{\pd \vec{\theta}}$
measuring how $\vec{\theta}$'s impact on $\h_k$ affects the cost
$\mathcal{E}_t$ at some future time $t > k$. The quantity
$\frac{\pd \h_t}{\pd \h_k}$ in \cref{eq:error-transfer} measures the affect of
the hidden state $\h_k$ on some future state $\h_t$ where $t > k$ and can be
interpreted as transferring the error ``in time'' from step $t$ back to step
$k$ \citep{Pascanu2012}.

\begin{figure}[tb]
    \centering
    \input{Chapter2/Figs/rnn-bptt.pdf_tex}
    \caption{The gradients passed along network edges during BPTT.}
    \label{fig:rnn-bptt}
\end{figure}

Just like traditional backpropagation, \cref{fig:rnn-bptt} demonstrates how
BPTT divides the computation of a global gradient $\frac{\pd \mathcal{E}}{\pd
\theta}$ into a series of local gradient computations, each of which involves
significantly less variables and hence is easier to compute.

\subsubsection{Vanishing/exploding gradients}

Unfortunately, naive implementations of RNNs (specifically
\cref{eq:rnn-dynamics}) often suffer from two well known problems: the
\emph{vanishing gradient} and \emph{exploding gradient} \citep{Bengio1994}.
These problems are both related to the product in \cref{eq:error-transfer}
exponentially growing or shrinking over long timespans (\ie $t \gg k$). A
sufficient condition (proved in \vref{sec:vanishing-exploding-gradients}) for
vanishing gradients is
\begin{equation}\label{eq:vanishing-gradients-suff}
  \left\| \W_{hh} \right\| < \frac{1}{\gamma_\sigma}
\end{equation}
where $\| \cdot \|$ is the matrix operator norm (see \vref{eq:operator-norm}),
$\W_{hh}$ is defined in \vref{eq:rnn-dynamics},
and $\gamma_\sigma$ is a constant depending on choice of activation function
(\eg $\gamma_\sigma = 1$ for $\sigma_{hh} = \tanh$, $\gamma_\sigma = 0.25$ for
$\sigma_{hh} = \sigmoid$).

This difficulty learning relationships between events spaced far apart in time
presents a signficant challenge for music applications. As noted by
\citet{cooper1963rhythmic}:
\begin{quote}
  Long-term dependencies are at the heart of what defines a style of music, with
  events spanning several notes or bars contributing to the formation of metrical and phrasal
  structure.
\end{quote}

\subsection{Long short term memory: solving the vanishing gradient}\label{sec:LSTM}

\nomenclature[z-LSTM]{LSTM}{Long Short Term Memory}
\nomenclature[z-CEC]{CEC}{Constant Error Carousel}
\nomenclature[r-input-gate]{$\i_t$}{Input gate values}
\nomenclature[r-forget-gate]{$\f_t$}{Forget gate values}
\nomenclature[r-output-gate]{$\o_t$}{Output gate values}
\nomenclature[r-Wxh]{$\W_{xh}$}{Weight matrix between input and hidden state}
\nomenclature[r-Wxh]{$\W_{hh}$}{Weight matrix between previous and current and hidden states}
\nomenclature[r-Wxh]{$\W_{hy}$}{Weight matrix between hidden state and output}
\nomenclature[g-sxh]{$\sigma_{xh}$}{Activation function between input and hidden state}
\nomenclature[g-shh]{$\sigma_{hh}$}{Activation function between previous and current hidden states}
\nomenclature[g-shy]{$\sigma_{hy}$}{Activation function between hidden state and output}

In order to build a model which learns long range dependencies, vanishing
gradients must be avoided. A popular memory cell architecture which does so is
\emph{long short term memory} (LSTM). Proposed by \citet{hochreiter1997long},
LSTM solves the vanishing gradient problem by enforcing \emph{constant error
flow} on \cref{eq:error-transfer}, that is
\begin{equation}\label{eq:const-err-flow}
    \W_{hh}^\tp \sigma_{hh}' (\h_{t}) = \matr{I}
\end{equation}
where $\matr{I}$ is the identity matrix.

As a result of the constant error flow condition, notice that \vref{eq:error-transfer}
becomes
\begin{equation}
  \frac{\pd \h_t}{\pd \h_k}
  = \prod_{t \geq i > k} \W_{hh}^\tp \diag \left( \sigma_{hh}'( \h_{i-1} ) \right)
  = \prod_{t \geq i > k} \matr{I}
  = \matr{I}
\end{equation}
The dependence on the time-interval $t-k$ is no longer present, ameliorating
the exponential decay causing vanishing gradients and enabling long-range
dependencies (\ie $t \gg k$) to be learned.

Integrating \cref{eq:const-err-flow} yields $\W_{hh} \sigma_{hh}(\h_{t}) = \h_{t}$.
Since this must hold for any hidden state $\h_{t}$, this means that:
\begin{enumerate}
    \item $\W_{hh}$ must be full rank
    \item $\sigma_{hh}$ must be linear
    \item $\W_{hh} \sigma_{hh} = \matr{I}$
\end{enumerate}

In the \emph{constant error carousel} (CEC), this is ensured by setting
$\sigma_{hh} = \W_{hh} = \I$. This may be interpreted as removing time dynamics
on $\h$ in order to permit error signals to be transferred backwards in time
(\cref{eq:error-transfer}) without modification (\ie $\forall t \geq k: \frac{\pd
\h_t}{\pd \h_k} = \I$).

In addition to using a CEC, a LSTM introduces three gates controlling access to the CEC:
\begin{description}
  \item[Input gate]: scales input $\x_t$ elementwise by $\i_t \in [0,1]$, \emph{writes} to $\h_t$
  \item[Output gate]: scales output $\y_t$ elementwise by $\o_t \in [0,1]$, \emph{reads} from $\h_t$
  \item[Forget gate]: scales previous cell value $\h_{t-1}$ by $\f_t \in [0,1]$, \emph{resets} $\h_t$
\end{description}

Mathematically, the LSTM model is defined by the following set of equations:
\begin{align}
    \i_t &= \sigmoid(\W_{xi} \x_t + \W_{yi} \y_{t-1} + \b_i) \\
    \o_t &= \sigmoid(\W_{xo} \x_t + \W_{yo} \y_{t-1} + \b_o) \\
    \f_t &= \sigmoid(\W_{xf} \x_t + \W_{yf} \y_{t-1} + \b_f) \\
    \h_t &= \f_t \odot \h_{t-1} + \i_t \odot \tanh(\W_{xh}\x_t + y_{t-1} \W_{yh} + \b_h) \\
    \y_t &= \o_t \odot \tanh(\h_t)
\end{align}
where $\odot$ denotes elementwise multiplication of vectors.

Notice that the gates ($\i_t$, $\o_t$, and $\f_t$) controlling flow in and out
of the CEC are all time varying. This can be interpreted as a mechanism
enabling LSTM to explicitly learn which error signals to trap in the CEC and
when to release them \citep{hochreiter1997long}, enabling error signals to
potentially be transported across long time lags.

\begin{figure}[tb]
    \centering
    \input{Chapter2/Figs/lstm-unit-2.pdf_tex}
    \caption{Schematic for a single LSTM memory cell. Notice how the gates $\i_t$, $\o_t$, and $\f_t$ control access to the constant error carousel (CEC).}
    \label{fig:lstm-cell}
\end{figure}

Some authors define LSTM such that $\h_t$ is not used to compute gate
activations, referring to the case where $\h_t$ is connected as ``peephole
connections'' \citep{gers2000recurrent}. We will use LSTM to refer to the
system of equations as written above.

\subsubsection{Practicalities for successful applications of LSTM}

Many applications of LSTM \mynote{cite examples} share some common practical
techniques for ensuring successful training. Perhaps most important is
\emph{gradient norm clipping} \citep{Mikolov2012,Pascanu2012} where the
gradient is rescaled whenever it exceeds a threshold. This is necessary because
while vanishing gradients are mitigated by the use of CECs, LSTM do not
explicitly protect against exploding gradients.

Another common practice is the use of methods for reducing overfitting and
improving generalization. In particular, \emph{dropout}
\citep{srivastava2014dropout} can be applied to the connections between memory
cells in a stacked RNN to regularize the learned features to be more robust to
noise \citep{zaremba2014recurrent}. Additionally, \emph{batch
normalization} \citep{ioffe2015batch} can also be applied to to the memory cell
hidden states to reduce co-variate shifts, accelerate training, and improve
generalization.

Finally, applications of RNNs to long sequences can incur a prohibitively high
cost for a single parameter update \citep{citeulike:13881859}. For instance,
computing the gradient of an RNN on a sequence of length $1000$ costs the
equivalent of a forward and backward pass on a $1000$ layer feedforward
network. This issue is typically addressed by only back-propogating error
signals a fixed number of timesteps back in the unrolled network, a technique
known as \emph{truncated BPTT} \citep{williams1990efficient}. As the hidden
states in the unrolled network have nevertheless been exposed to many timesteps,
learning of long range structure is still possible.
