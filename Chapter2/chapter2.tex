\chapter{Background}

\ifpdf
    \graphicspath{{Chapter2/Figs/Raster/}{Chapter2/Figs/PDF/}{Chapter2/Figs/}}
\else
    \graphicspath{{Chapter2/Figs/Vector/}{Chapter2/Figs/}}
\fi

The goal of this chapter is to provide only the necessary background in
recurrent neural networks and generative probabilistic sequence modelling
required for understanding our models, experiments, and results. It also
introduces some common definitions and clarifies notation used throughout later chapters.

A basic understanding of Western music theory and neural networks is assumed.
Readers unfamiliar with concepts such as piano rolls, Roman numeral analysis,
and cadences, should review \vref{sec:music-theory-primer} for a quick primer
and \citet{piston1978harmony,denny1960oxford} for more thorough coverage.
Likewise, those whom wish to review concepts such as activation functions,
neurons, and applying recurrent neural networks over arbitrary length sequences
are advised to review \vref{sec:primer-nn} and consult
\citet{bengio2009learning} for further reference.

\section{Recurrent neural networks}\label{sec:bg-rnn}


\subsection{Notation}

\nomenclature[z-RNN]{RNN}{Recurrent Neural Network}
\nomenclature[a-input]{$\x$}{layer inputs}
\nomenclature[a-hidden-state]{$\h$}{hidden state (\ie memory cell contents)}
\nomenclature[a-output]{$\y$}{layer outputs}
\nomenclature[a-Wxh]{$\W$}{weight matrix}
\nomenclature[a-Nin]{$N_{in}$}{dimensionality of inputs}
\nomenclature[a-Nhid]{$N_{hid}$}{dimensionality of hidden state}
\nomenclature[a-Nout]{$N_{out}$}{dimensionality of outputs}
\nomenclature[a-T]{$T$}{total number of timesteps in a sequence}
\nomenclature[g-sxh]{$\sigma$}{elementwise activation function}
\nomenclature[g-th]{$\theta$}{Model Parameters}
\nomenclature[r-l]{$(l)$}{layer index in multi-layer networks}
\nomenclature[s-t]{$t$}{time index}
\nomenclature[s-st]{$st$}{connections from source $s$ to target $t$}

We begin by clarifying common notation and conventions used to describe
multi-layer \emph{recurrent neural networks} (RNNs). Unless otherwise
specified, future use of notation should be interpreted as defined in this
section.

We use the subscript $t \in \{ 1,2,\cdots,T \}$ to denote the \emph{time index}
within a sequence of length $T \in \NN$

A sequence of \emph{inputs} is denoted by $\x$ and the sequence elements at timestep $t$ is
denoted by $\x_t \in \RR^{N_{in}}$ and assumed to have dimensionality $N_{in} \in \NN$.
Similarly, $\h_t \in \RR^{N_{hid}}$ and $\y_t \in \RR^{N_{out}}$ denote elements
from the \emph{hidden state} and \emph{output} sequences respectively.

To describe model parameters, we use $\W$ to indicate a real-valued
\emph{weight matrix} consisting of all the connection weights between two sets
of neurons and $\sigma(\cdot)$ to indicate an elementwise \emph{activation
function}. The collection of all model parameters is denoted by $\vec{\theta}$.

When further clarity is required, we use subscripts $\W_{st}$ denote the
connection weights from a set of neurons $s$ to another set of neurons $t$ (\ie
in \vref{sec:LSTM}, $\W_{xf}$ and $\W_{xh}$ refer to the connections from the
inputs to the forget gate and hidden state respectively). Subscripts on
activation functions $\sigma_{s,t}(\cdot)$ are to be interpreted analogously.

Equipped with the above notation, the equations for \emph{RNN time dynamics} (review
\vref{sec:primer-nn} if this is unfamiliar) can be expressed as
\begin{equation}\label{eq:rnn-dynamics}
 \left.\begin{aligned}
          \h_t &=& \W_{xh} \sigma_{xh} \left( \x_t \right) + \W_{hh} \sigma_{hh} \left( \h_{t-1} \right)\\
          \y_t &=& \W_{hy} \sigma_{hy} \left( \h_t \right)
       \end{aligned}
 \right\}
 \qquad \text{RNN time dynamics}
\end{equation}

When discussing multi-layer networks, we use $L \in \NN$ to denote total number
of layers and parenthesized superscripts $(l)$ for $l \in \{1,2,\cdots,L\}$ to
indicate the layer. For example, $\z^{(2)}_t$ is the hidden states of the
second layer and $N^{(3)}_{in}$ is the dimensionality of the third layer's
inputs $\x^{(3)}_t$. Unless stated otherwise, multi-layer networks will assume
that the outputs of the $l-1$st layer are used as the inputs of the $l$th layer
(\ie $\forall t: \x^{(l)}_t = \y^{(l-1)}_t$).

\subsection{The memory cell abstraction}

While a large number of proposed RNN variants exist
\citep{elman1990finding,jordan1997serial,hochreiter1997long,cho2014learning,Koutnik2014,Mikolov2015},
most share the same underlying structure and differ only in small localized
implementation details. Encapsulating these differences and abstracting them
away enables general discussion about RNN architecture without making a
specific choice on implementation.

To do so, we introduce the \emph{memory cell} abstraction to encapsulate the
details of computing $\y_t$ and $\h_t$ from $\x_t$ and $\h_{t-1}$. This is
illustrated visually in \cref{fig:rnn-elman}, which shows a standard Elman-type
RNN \citep{elman1990finding} with the memory cell indicated by a dashed box
isolating the recurrent hidden state. The edges entering the memory cell
($\x_t$, $\h_{t-1}$) make up the \emph{memory cell inputs} and the outgoing
edges ($\y_t$, $\h_t$) are the \emph{memory cell outputs}, so a concrete
implementation of a memory cell needs to provide two functions $\f_h$ and
$\f_y$ which uses $\x_t$ and $\h_{t-1}$ to compute the next hidden state $\h_t
= f_h(\x_t, \h_{t-1})$ and output $\y_t = f_y(\h_t)$.

\begin{figure}[tb]
  \centering
  \input{Chapter2/Figs/nn-rnn-elman.pdf_tex}
  \caption{An Elman-type RNN with a single hidden layer. The recurrent hidden
    state is illustrated as unit-delayed (denoted by $z^{-1}$) feedback edges
    from the hidden states to the input layer. The memory cell encapsulating the
  hidden state is also shown.}
  \label{fig:rnn-elman}
\end{figure}

\subsection{Operations on RNNs: stacking and unrolling}

\subsubsection{Stacking memory cells to form deep RNNs}

Just like deep neural networks, RNNs can be \emph{stacked} to form deep RNNs
\citep{el1995hierarchical,schmidhuber1992learning} by treating the
outputs from the $l-1$st layer's memory cells as inputs to the $l$th layer (see \cref{fig:rnn-multi-unrolled}).

\begin{figure}[tb]
    \centering
    \resizebox{4.5in}{!}{\input{Chapter2/Figs/rnn-multi-unrolled.pdf_tex}}
    \caption{Block diagram representation of a -layer RNN (left) and its
    corresponding DAG (right) after unrolling. The blocks labelled
    with $\h_t$ represent memory cells whose parameters are shared across all times
  $t$.}
    \label{fig:rnn-multi-unrolled}
\end{figure}

Prior work has observed that ``deep RNNs outperformed the conventional, shallow RNN''
\citet{pascanu2013construct}, affirming the importance of stacking multiple layers
in RNNs. The improved modelling expressivity can be attributed to two primary
factors: composition of multiple non-linear activation functions and an
increase in the number of paths for backpropagated error signals to flow. The
former reason is analogous to the case in deep belief networks, which is well
documented \citep{bengio2009learning}. To understand the latter, notice that in
\cref{fig:rnn-multi-unrolled} there is only a single path from $\x_{t-1}$ to
$\y_{t}$ hence the conditional independence $\y_{t} \independent \x_{t-1} |
\h^{(1)}_t$ is satisfied. However, in \cref{fig:rnn-multi-unrolled} there are
multiple paths from $\x_{t-1}$ to $\y_{t}$ (\eg passing through either
$\h^{(2)}_{t-1} \to \h^{(2)}_t$ or $\h^{(1)}_{t-1} \to \h^{(1)}_t$) through
which information may flow.

\subsubsection{Unrolling RNNs into directed acyclic graphs}

\nomenclature[z-DAG]{DAG}{Directed Acyclic Graph}

Given an input sequence $\{\x\}_{t=1}^T$, an RNN can be \emph{unrolled} into a
\emph{directed acyclic graph} (DAG) comprised of $T$ copies of the memory cell
connected forwards in time. This is illustrated for a stacked 2-layer RNN in
\Cref{fig:rnn-multi-unrolled}, where the vectors $\y_t$, $\h_t$, and $\x_t$ are
depicted as blocks and the $\h_t$ is understood to represent a memory cell.

% \begin{figure}[tb]
%   \centering
%   \resizebox{4.5in}{!}{\input{Chapter2/Figs/rnn-single-unrolled.pdf_tex}}
%   \caption{Signal flow diagram representation of a single-layer RNN (left) and its
%     corresponding DAG (right) after unrolling. The blocks labelled
%     with $\h_t$ represent memory cells whose parameters are shared across all times
%   $t$.}
%   \label{fig:rnn-single-unrolled}
% \end{figure}

\Cref{fig:rnn-multi-unrolled} shows that the hidden state $\h_t$ is passed
forwards throughout the sequence of computations. This gives rise to an
alternative interpretation of the hidden state as a temporal memory mechanism.
Under this interpretation, updating the hidden state $\h_t = f_h (\x_t,
\h_{t-1})$ can be viewed as \emph{writing} information from the current inputs
$\x_t$ to memory and producing the outputs $\y_t = f_y (\h_t)$ can be
interpreted as \emph{reading} information from memory.


% Additionally, parameters need not be shared
% across different layers so the stacked RNN can learn different time dynamics
% for each layer.

\subsection{Training RNNs and backpropagation through time}

\nomenclature[z-BPTT]{BPTT}{Backpropagation Through Time}
\nomenclature[r-E]{$\mathcal{E}$}{Error or Loss}
\nomenclature[r-Et]{$\mathcal{E}_t$}{Error or Loss at time $t$}

The parameters $\vec{\theta}$ of a RNN are typically learned from data by
minimizing some \emph{cost} $\mathcal{E} = \sum_{1 \leq t \leq T}
\mathcal{E}_t(\x_t)$ measuring the performance of the network on some task.
This optimization is usually performed using iterative methods which require
computation of gradients $\frac{\pd \mathcal{E}}{\pd \vec{\theta}}$ at each
iteration.

In feed-forward networks, computation of gradients can be performed efficiently
using backpropagation
\citep{bryson1963optimal,linnainmaa1970representation,rumelhart1988learning}.
While time-delayed recurrent hidden state connections appear to complicate
matters initially, unrolling the RNN removes the time-delayed recurrent edges
and converts the RNN into a DAG (\eg \vref{fig:rnn-multi-unrolled}) which can
be interpreted as a $T$ layered feed-forward neural network with parameters
shared across all $T$ layers.

This view of unrolled RNNs as feedforward networks motivates
\emph{backpropagation through time} (BPTT) \citep{goller1996learning}, a method
for training RNNs which applies backpropagation to the unrolled DAG. However,
the unrolled RNN can possess an extremely large number of layers if $T$ is
large, introducing problems not encountered when training feedforward networks
with fixed depth.

To illustrate the problem, we can first apply the chain rule to the RNN
dynamics equations (\vref{eq:rnn-dynamics}) unrolled network (see
\cref{fig:rnn-bptt}), we obtain
\begin{align}
  \frac{\pd \mathcal{E}}{\pd \vec{\theta}} &= \sum_{1 \leq t \leq T} \frac{\pd \mathcal{E}_t}{\pd \vec{\theta}} \label{eq:err-total}\\
    \frac{\pd \mathcal{E}_t}{\pd \vec{\theta}} &= \sum_{1 \leq k \leq t} \left(
        \frac{\pd \mathcal{E}_t}{\pd \y_t}
        \frac{\pd \y_t}{\pd \h_t}
        \frac{\pd \h_t}{\pd \h_k}
        \frac{\pd \h_k}{\pd \vec{\theta}}
    \right) \label{eq:error-t}\\
    \frac{\pd \h_t}{\pd \h_k} &=
    \prod_{t \geq i > k} \frac{\pd \h_i}{\pd \h_{i-1}}
    = \prod_{t \geq i > k} \W_{hh}^\tp \diag \left( \sigma_{hh}'( \h_{i-1} ) \right)
    \label{eq:error-transfer}
\end{align}

\Cref{eq:error-t} expresses how the error $\mathcal{E}_t$ at time $t$ is a sum
of \emph{temporal contributions} $
\frac{\pd \mathcal{E}_t}{\pd \y_t}
\frac{\pd \y_t}{\pd \h_t}
\frac{\pd \h_t}{\pd \h_k}
\frac{\pd \h_k}{\pd \vec{\theta}}$
measuring how $\vec{\theta}$'s impact on $\h_k$ affects the cost
$\mathcal{E}_t$ at some future time $t > k$. The quantity
$\frac{\pd \h_t}{\pd \h_k}$ in \cref{eq:error-transfer} measures the affect of
the hidden state $\h_k$ on some future state $\h_t$ where $t > k$ and can be
interpreted as transferring the error ``in time'' from step $t$ back to step
$k$ \citep{Pascanu2012}.

\begin{figure}[tb]
    \centering
    \input{Chapter2/Figs/rnn-bptt.pdf_tex}
    \caption{The gradients passed along network edges during BPTT.}
    \label{fig:rnn-bptt}
\end{figure}

Just like traditional backpropagation, \cref{fig:rnn-bptt} demonstrates how
BPTT divides the computation of a global gradient $\frac{\pd \mathcal{E}}{\pd
\theta}$ into a series of local gradient computations, each of which involves
significantly less variables and hence is easier to compute.

\subsubsection{Vanishing/exploding gradients}

Naive implementations of RNNs (specifically \cref{eq:rnn-dynamics}) often
suffer from two well known problems: the \emph{vanishing gradient} and
\emph{exploding gradient} \citep{Bengio1994}. These problems are both related
to the product in \cref{eq:error-transfer} exponentially growing or shrinking
over long time-spans (\ie $t \gg k$). A sufficient condition (proved in
\vref{sec:vanishing-exploding-gradients}) for vanishing gradients is
\begin{equation}\label{eq:vanishing-gradients-suff}
  \left\| \W_{hh} \right\| < \frac{1}{\gamma_\sigma}
\end{equation}
where $\| \cdot \|$ is the matrix operator norm (see \vref{eq:operator-norm}),
$\W_{hh}$ is defined in \vref{eq:rnn-dynamics},
and $\gamma_\sigma$ is a constant depending on the choice of activation function
(\eg $\gamma_\sigma = 1$ for $\sigma_{hh} = \tanh$, $\gamma_\sigma = 0.25$ for
$\sigma_{hh} = \sigmoid$).

This difficulty learning relationships between events spaced far apart in time
presents a significant challenge for music applications. As noted by
\citet{cooper1963rhythmic}:
\begin{quote}
  Long-term dependencies are at the heart of what defines a style of music, with
  events spanning several notes or bars contributing to the formation of metrical and phrasal
  structure.
\end{quote}

\subsection{Long short term memory: solving the vanishing gradient}\label{sec:LSTM}

\nomenclature[z-LSTM]{LSTM}{Long Short Term Memory}
\nomenclature[z-CEC]{CEC}{Constant Error Carousel}
\nomenclature[r-input-gate]{$\i_t$}{Input gate values at time $t$}
\nomenclature[r-forget-gate]{$\f_t$}{Forget gate values at time $t$}
\nomenclature[r-output-gate]{$\o_t$}{Output gate values at time $t$}
\nomenclature[x-odot]{$\odot$}{Elementwise multiplication}

In order to build a model which learns long range dependencies, vanishing
gradients must be avoided. A popular memory cell architecture which does so is
\emph{long short term memory} (LSTM). Proposed by \citet{hochreiter1997long},
LSTM solves the vanishing gradient problem by enforcing \emph{constant error
flow} on \cref{eq:error-transfer}, that is
\begin{equation}\label{eq:const-err-flow}
    \W_{hh}^\tp \sigma_{hh}' (\h_{t}) = \matr{I}
\end{equation}
where $\matr{I}$ is the identity matrix.

As a result of the constant error flow condition, notice that \vref{eq:error-transfer}
becomes
\begin{equation}
  \frac{\pd \h_t}{\pd \h_k}
  = \prod_{t \geq i > k} \W_{hh}^\tp \diag \left( \sigma_{hh}'( \h_{i-1} ) \right)
  = \prod_{t \geq i > k} \matr{I}
  = \matr{I}
\end{equation}
The dependence on the time-interval $t-k$ is no longer present, ameliorating
the exponential decay causing vanishing gradients and enabling long-range
dependencies (\ie $t \gg k$) to be learned.

Integrating \cref{eq:const-err-flow} yields $\W_{hh} \sigma_{hh}(\h_{t}) = \h_{t}$.
Since this must hold for any hidden state $\h_{t}$, this means that:
\begin{enumerate}
    \item $\W_{hh}$ must be full rank
    \item $\sigma_{hh}$ must be linear
    \item $\W_{hh} \sigma_{hh} = \matr{I}$
\end{enumerate}

In the \emph{constant error carousel} (CEC), this is ensured by setting
$\sigma_{hh} = \W_{hh} = \I$. This may be interpreted as removing time dynamics
on $\h$ in order to permit error signals to be transferred backwards in time
(\cref{eq:error-transfer}) without modification (\ie $\forall t \geq k: \frac{\pd
\h_t}{\pd \h_k} = \I$).

In addition to using a CEC, a LSTM introduces three gates controlling access to the CEC:
\begin{description}
  \item[Input gate]: scales input $\x_t$ elementwise by $\i_t \in [0,1]$, \emph{writes} to $\h_t$
  \item[Output gate]: scales output $\y_t$ elementwise by $\o_t \in [0,1]$, \emph{reads} from $\h_t$
  \item[Forget gate]: scales previous cell value $\h_{t-1}$ by $\f_t \in [0,1]$, \emph{resets} $\h_t$
\end{description}

Mathematically, the LSTM model is defined by the following set of equations:
\begin{align}
    \i_t &= \sigmoid(\W_{xi} \x_t + \W_{yi} \y_{t-1} + \b_i) \\
    \o_t &= \sigmoid(\W_{xo} \x_t + \W_{yo} \y_{t-1} + \b_o) \\
    \f_t &= \sigmoid(\W_{xf} \x_t + \W_{yf} \y_{t-1} + \b_f) \\
    \h_t &= \f_t \odot \h_{t-1} + \i_t \odot \tanh(\W_{xh}\x_t + y_{t-1} \W_{yh} + \b_h) \\
    \y_t &= \o_t \odot \tanh(\h_t)
\end{align}
where $\odot$ denotes elementwise multiplication of vectors.

Notice that the gates ($\i_t$, $\o_t$, and $\f_t$) controlling flow in and out
of the CEC are all time varying. This can be interpreted as a mechanism
enabling LSTM to explicitly learn which error signals to trap in the CEC and
when to release them \citep{hochreiter1997long}, enabling error signals to
potentially be transported across long time lags.

\begin{figure}[tb]
    \centering
    \input{Chapter2/Figs/lstm-unit-2.pdf_tex}
    \caption{Schematic for a single LSTM memory cell. Notice how the gates $\i_t$, $\o_t$, and $\f_t$ control access to the constant error carousel (CEC).}
    \label{fig:lstm-cell}
\end{figure}

Some authors define LSTM such that $\h_t$ is not used to compute gate
activations, referring to the case where $\h_t$ is connected as ``peephole
connections'' \citep{gers2000recurrent}. We will use LSTM to refer to the
system of equations as written above.

\subsubsection{Practicalities for successful applications of LSTM}

Many applications of LSTM
\citep{devlin2014fast,zaremba2015empirical,pascanu2013construct} share some
common practical techniques for ensuring successful training. Perhaps most
important is \emph{gradient norm clipping} \citep{Mikolov2012,Pascanu2012}
where the gradient is scaled or clipped whenever it exceeds a threshold. This
is necessary because while vanishing gradients are mitigated by CECs, LSTM do
not explicitly protect against exploding gradients.

Another common practice is the use of methods for reducing overfitting and
improving generalization. In particular, \emph{dropout}
\citep{srivastava2014dropout} can be applied to the connections between memory
cells in a stacked RNN to regularize the learned features to be more robust to
noise \citep{zaremba2014recurrent}. Additionally, \emph{batch normalization}
\citep{ioffe2015batch} can also be applied to to the memory cell hidden states
to reduce co-variate shifts, accelerate training, and improve generalization.

Finally, applications of RNNs to long sequences can incur a prohibitively high
cost for a single parameter update \citep{citeulike:13881859}. For instance,
computing the gradient of an RNN on a sequence of length $1000$ costs the
equivalent of a forward and backward pass on a $1000$ layer feed-forward
network. This issue is typically addressed by only backpropagating error
signals a fixed number of timesteps back in the unrolled network, a technique
known as \emph{truncated BPTT} \citep{williams1990efficient}. As the hidden
states in the unrolled network have nevertheless been exposed to many
timesteps, learning of long range structure is still possible.

\section{Sequence probability modelling}

\mynote{Introduce this stuff better (should we assume LSTM after this point)}

In order to use LSTM as a model for music, the following assumptions
about the sequences $\x_{1:T}$, $\y_{1:T}$, and $\h_{0:T}$ are made:
\begin{enumerate}
  \item Modified Markov assumption:
    \begin{equation}
      \label{eq:modified-markov}
      \forall t: P(\h_t | \h_{0:t-1}, \x_{1:t}) = P(\h_t | \h_{t-1}, \x_t)
    \end{equation}
  \item Hidden State Stationarity:
    \begin{equation}
      \label{eq:hidden-state-stationarity}
      \forall t_1, t_2: P(\h_{t_1} = \k | \h_{t_{1}-1} = \i, \x_{t_1} = \j) = P(\h_{t_2} = \k | \h_{t_{2}-1} = \i, \x_{t_2} = \j)
   \end{equation}
  \item Output Stationarity:
    \begin{equation}
      \label{eq:output-stationarity}
      \forall t_1, t_2: P(\y_{t_1} = \j | \h_{t_1} = \i) = P(\y_{t_2} = \j | \h_{t_2} = \i)
   \end{equation}
  \item Output independence:
   \begin{equation}
     \label{eq:output-independence}
     P(\y_{1:T} | \h_{0:T}, \x_{1:T}) = \prod_{t=1}^T P(\y_t | \h_{t}, \x_t)
   \end{equation}
\end{enumerate}

These assumptions imply the sequential factorization:
\begin{align}
  &P(\y_{1:T}, \h_{1:T} | \h_0, \x_{1:T})  &\\
  &= P(\y_{1:T} | \h_{0:T}, \x_{1:T}) P(\h_{1:T} | \h_0, \x_{1:T})  &\\
  &= \left( \prod_{t=1}^T P(\y_t | \h_t) \right) P(\h_{1:T}| \h_0, \x_{1:T}) &\text{\cref{eq:output-independence}}\\
  &= \left( \prod_{t=1}^T P(\y_t | \h_t) \right) \left(\prod_{t=1}^T P(\h_{t}| \h_{0:t-1}, \x_{1:t})\right) &\\
  &= \left( \prod_{t=1}^T P(\y_t | \h_t) \right) \left(\prod_{t=1}^T P(\h_{t}| \h_{t-1}, \x_{t})\right) &\text{\cref{eq:modified-markov}}\\
  &= \prod_{t=1}^T P(\y_t | \h_t, \x_t) P(\h_{t}| \h_{t-1}, \x_{t}) & \\
\end{align}
Together, \cref{eq:hidden-state-stationarity} and \cref{eq:output-stationarity} imply that $P(\y_t | \h_t)$
and $P(\h_{t}| \h_{t-1}, \x_{t})$ are time-invariant and can be modelled by the same recurrent function.

In RNNs, the hidden state dynamics $P(\h_t | \h_{t-1}, \x_t)$ are deterministic:
\begin{equation}
  \h_t = f_h(\x_t, \h_{t-1})
\end{equation}
Which means that $P(\y_{1:T}, \h_{1:T} | \h_0, \x_{1:T}) = P(\y_{1:T} | \h_0, \x_{1:T})$.
This yields the factorization
\begin{equation}
  P(\y_{1:T} | \h_0, \x_{1:T})
  = P(\y_{1:T}, \h_{1:T} | \h_0, \x_{1:T})
  = \prod_{t=1}^T P(\y_t | \h_t, \x_t) f_h(\x_t, \h_{t-1})
\end{equation}
\mynote{Draw PGM}

However, one minor problem remains. Let $\z_t = f_y(f_h(\x_t, \h_{t-1}))$ (with
$f_y$ and $f_h$ as defined in \mynote{ref}) denote the outputs of the RNN model
at time $t$. Note that $\z_t$ can be any real vector in $\RR^{|V|}$
\mynote{Define $V$ to be the vocabulary}, but $P(\x_{t+1} | \h_{t-1}, \x_{t})$ is
a probability vector constrained to sum to one.

Fortunately, we can treat $\z_t$ as the \emph{scores} for a \emph{Boltzmann
distribution}
\begin{equation}\label{eq:boltzmann-dist}
    P( \y_{t} = s | \h_{t-1}, \x_t )
    = \frac{\exp \left(-\z_{t,s}/T\right) }{ \sum_{k=1}^{K} \left(\exp -\z_{t,k}/T\right)}
\end{equation}
where $T \in \RR^+$ is a \emph{temperature} parameter (set to $T=1$ during training and varied during sampling).
To keep notation compact, we omit writing this explicitly and understand $P(\y_t | \h_{t-1}, \x_t)$ to mean
the Boltzmann distribution parameterized by the scores $f_y(f_h(\x_t, \h_{t-1}))$.

Note the similarity between \cref{eq:modified-markov}--\cref{eq:output-independence}
and the assumptions for Hidden Markov models \citep{ramage2007hidden}. Discrepancies are due
to the presence of an input sequence $\x_{1:T}$ in our sequence-to-sequence model.

\mynote{Discuss validity of assumptions, namely output independence assuming hidden state and input summarize
all prior context}


