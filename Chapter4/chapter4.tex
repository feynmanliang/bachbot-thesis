\chapter{Automatic composition with deep LSTM}
\begin{savequote}[75mm]
  We find ourselves in front of an attempt, as objective as possible, of creating
  an automated art, without any human interference except at the start, only in
  order to give the initial impulse and a few premises, like in the case
  of\ldots nothingness in the Big Bang Theory
  \qauthor{\citet{hoffmann2002towards}}
\end{savequote}
% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter4/Figs/Raster/}{Chapter4/Figs/PDF/}{Chapter4/Figs/}}
\else
    \graphicspath{{Chapter4/Figs/Vector/}{Chapter4/Figs/}}
\fi

This chapter describes a generative RNN sequence model for polyphonic music. We
first describe our process for constructing a training corpus of chorales and
quantify the impact of our preprocessing procedure over the corpus. Next, we
present a simple frame-based sequence encoding for polyphonic music. Using this
sequence representation, we perform an investigation of various RNN
architectures, design tradeoffs, and training methods. The results of our
investigation are used to design our final model, which we compare against
prior work and utilize in following chapters.

\section{Constructing a corpus of encoded scores}

We restrict the scope of our investigation to Bach chorales for the following reasons:
\begin{enumerate}
  \item The Baroque style employed in Bach chorales has specific guidelines
    \citep{piston1978harmony} (i.e.\ no parallel fifths) and stylistic elements
    (i.e. voice leading) which can be use to qualitatively evaluate success
  \item The structure of chorales are regular: all chorales have four parts and
    consist of a melody in the Soprano part harmonized by the Alto, Tenor, and
    Bass parts. Additionally, each chorale consists of a series of \emph{phrases}:
    groupings of consecutive notes into a unit that has complete musical sense
    of its own\citep{nattiez1990music}. It is well known\mynote{citep} that Bach
    denoted ends of phrases with fermatas\mynote{refer back to background}.
  \item The Bach chorales have become a standardized corpus routinely studied
    by aspiring music theorists\citep{white2002guidelines}
\end{enumerate}
The Bach chorales, indexed by the Bach-Werke-Verzeichnis (BWV) numbering
system\citep{butt1999bach}, are conveniently provided by
\texttt{music21}\citep{Scott2015}.

\subsection{Preprocessing}

Motivated by the transposition invariance of music and prior practice
\citep{mozer1994neural} \citep{Eck2002} \citep{franklin2004recurrent}
\citep{franklin2005jazz}, we first perform \emph{key normalization}.
The key signature of each score were firsanalyzed using the Krumhansl
Schmuckler key-finding algorithm \citep{krumhansl2001cognitive} and then
transposed according to \mynote{Table XYZ} such that the transposed key is
C-major for major mode scores and A-minor for minor mode scores.

Next, we perform \emph{time quantization} by aligning note start and end times
to the nearest multiple of some minimum duration. Our model uses a minimum
duration of one $1/16$th note, exceeding the time resoltuions of
\citep{Boulanger-Lewandowski2012} \citep{Eck2002} by 2x,
\citep{hild1991harmonet} by 4x, and \citep{bellgard1994harmonizing} by 8x.

We consider only note pitches and durations, neglecting changes in timing
(e.g. ritardandos), dynamics (e.g. crescendos), and stylistic notations (e.g.
accents, staccatos, legatos).

An example of the distortion introduced through of our preprocessing steps is
provided in \vref{fig:score-effects-preproc} in sheet music notation and in
piano roll\mynote{Is piano roll defined?} notation on
\vref{fig:piano-roll-effects-preproc}.
\begin{figure}[p]
    \centering
    \includegraphics[width=0.8\linewidth]{bwv185-6-original-score-1.png}

    \vspace{1.5cm}

    \includegraphics[width=0.8\linewidth]{bwv185-6-preproc-score-1.png}
    \caption{First 4 bars of JCB Chorale BWV 133.6 before (top) and after (bottom) preprocessing. Note
    the transposition down by a semitone to C-major as well as quantization of the
    demisemiquavers in the third bar of the Soprano part.}
    \label{fig:score-effects-preproc}
\end{figure}

\begin{figure}[p]
    \centering
        \input{Chapter4/Figs/bwv185-6-original-piano-roll.pgf}
        \input{Chapter4/Figs/bwv185-6-preproc-piano-roll.pgf}
    \caption{Piano roll representation of the same 4 bars from \cref{fig:score-effects-preproc}
      before and after preprocessing. Again, note the transposition to C-major
      and time-quantization occuring in the Soprano part.}
    \label{fig:piano-roll-effects-preproc}
\end{figure}

\subsubsection{Corpus level analysis of preprocessing effects}

To assess the effects introduced by key normalization and time quantization,
we analyze corpus level statistics related to pitch and duration.

\cref{fig:pitch-key-standardization} plots a histogram of pitch usage counts
before and after key normalization. Notice that the overall range of pitches
has increased after key normalization. This can be explained by noting that
Bach's chorales were to be performed by vocalists and hence were restricted to
use pitches within human voice ranges regardless of key. After transposition,
this constraint is no longer be satisfied and we see the appearance of
unrealistically low notes (e.g. A1) outside the range of even the lowest voice
types.

\begin{landscape}
  \begin{figure}[p]
    \centering
    \begin{subfigure}[c]{0.7\textwidth}
        \centering
        \input{Chapter4/Figs/pitch-usage-original.pgf}
        \input{Chapter4/Figs/pitch-usage-preproc.pgf}
    \end{subfigure}
    \begin{subfigure}[c]{0.28\textwidth}
        \centering
        \input{Chapter4/Figs/pitch-class-usage-original.pgf}
        \input{Chapter4/Figs/pitch-class-usage-preproc.pgf}
    \end{subfigure}
    \caption{Distribution of pitches used over Bach chorales corpus.
      Transposition has resulted in an overall broader range of pitches and
    increased the counts of pitches which are in key.}
    \label{fig:pitch-key-standardization}
  \end{figure}
\end{landscape}

In \cref{fig:pc-key-standardization}, we visualize histograms of pitch class usages.
As expected, key normalization has increased the usage of pitch classes in the key of
C-major / A-minor (i.e. those which possess no accidentals) and decreased out of key
pitch classes (e.g. C\#, F\#).

\begin{figure}
    \centering
    \caption{Distribution of pitch classes over Bach chorales corpus. Transposition has increased the counts
    for pitch classes within the C-major / A-minor scales.}
    \label{fig:pc-key-standardization}
\end{figure}

We investigate the effects of time quantization in
\cref{fig:note-lengths-time-quantization}, which shows histograms of note
duration usages before and after quantization. \mynote{Update plots... are they affected}.

\begin{figure}[tb]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \input{Chapter4/Figs/note-lengths-original.pgf}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \input{Chapter4/Figs/note-lengths-quantized.pgf}
    \end{subfigure}
    \caption{Distribution of note durations over Bach chorales corpus. Quantization has minimal impact
    because of the high resolution (semiquavers) used.}
    \label{fig:note-lengths-time-quantization}
\end{figure}

\begin{figure}[tb]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \input{Chapter4/Figs/meter-usage-original.pgf}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \input{Chapter4/Figs/meter-usage-quantized.pgf}
    \end{subfigure}
    \caption{Meter is minimally affected by quantization due to the high resolution used for
    time quantization.}
    \label{fig:meter-time-quantization}
\end{figure}

\subsection{Sequential encoding of musical data}
\label{sec:sequential-encoding}

After preprocessing of the scores, our next step is to encode music into a
sequence of tokens amenable for processing by RNNs. One design decision is
whether the tokens in the sequence are comprised of individual notes (as done
in \citep{mozer1994neural,franklin2004recurrent,sturm2016music}) or larger
harmonic units (e.g. chords \citep{Eck2002,Boulanger-Lewandowski2012},
``harmonic context'' \citep{Allan2005}). This tradeoff is similar to one faced
in RNN language modelling where either individual characters or entire words
can be used.

In contrast to most language models which operate at the word level, we choose
to construct our models at the note level for several reasons.
Firstly, the issue of multiple tokens in the sequence corresponding to the same
instant of time in the represented music should not be problematic because
LSTMs have been shown to be able to learn to implement precise timing and
counting\cite{gers2002learning}. Additionally, the use of a note-level encoding
partially mitigates the problem of out-of-vocabulary (OOV) tokens in two ways.
Besides reducing the potential vocabulary size from $O(128^4)$ possible chords
to $O(128)$ potential notes, the model is now able to capture harmonic
relationships between notes within the LSTM model weights ($\W_{xx}$,
$\W_{xh}$, $\W_{hh}$ in \mynote{reference}) and may generalize better to unseen
chords. Furthermore, \citet{graves2013generating} showed comparable performance
between LSTM language models that operate on individual characters versus words
(perplexities of $1.24$ bits vs $1.23$ bits per character respectively),
suggesting that the choice is not too significant at least for English language
modelling.


Similar to \citep{todd1989connectionist}, we represent polyphonic scores using
a localist frame-based representation where time is discretized into constant
timestep \emph{frames}. Frame based processing forces the network to learn the
relative duration of notes, a counting and timing task which
\citep{gers2002learning} demonstred LSTM is capable of. Consecutive frames are
separated by a unique delimiter (``$|||$''' in \mynote{Figure of score encoded
in text}).

Each frame consists of a sequence of $\langle \text{Note}, \text{Tie} \rangle$
tuples where $\text{Note} \in \{0,1,\cdots,127\}$ represents the MIDI pitch of
a note and $\text{Tie} \in \{T,F\}$ distinguishes whether a note is tied with a
note at the same pitch from the previous frame or is articulated at the current
timestep. A design decision here is the order in which notes within a frame are
encoded and consequentially processed by a sequential model. Since chorale
music places the melody in the Soprano part, it is reasonable to expect the
Soprano notes to be most significant in determining the other parts. Hence, we
choose to process the Soprano notes first and order notes in descending pitch
within every frame.

The above specification describes our initial encoding. Later in our work
\mynote{reference}, we found that this encoding resulted in unrealistically long
phrase lengths. Including fermatas (represented by ``(.)'' in \mynote{Figure of
encoded score}, which Bach used to denote ends of phrases, solves this problem.

Finally, for each score a unique start symbol (``START'' in \mynote{Figure})
and end symbol (``END'' in \mynote{Figure}) are appended to the beginning and
end repsectively. This causes the model to learn to initialize itself when
given the start symbol and allows us to determine when a composition generated
by the model has concluded. The vocabulary and corpus size after encoding is
detailed in \cref{tab:encoded-corpus-stats}. The rank-size distribution of the
note-level corpus tokens is shown in \cref{fig:zipf} and confirms the failure
of Zipf's law in our data.

\begin{table}[tb]
  \centering
  \caption{Statistics on the preprocessed datasets used throughout our study}
  \label{tab:encoded-corpus-stats}.
  \begin{tabular}{c c c c}
    \toprule
    Vocabulary size & Total \# tokens & Training size & Validation size \\
    \midrule
    108 & 423463 & 381117 & 42346 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[tb]
  \centering
  \input{Chapter4/Figs/zipf-law-note-tuples.pgf}
  \caption{Left: Token frequencies sorted by rank. Right: log-log plot where
  a power law distribution as predicted by Zipf's law would appear linear.}
  \label{fig:zipf}
\end{figure}

Notice that our encoding is sparse: unarticulated notes are not encoded. It is
also variable length as anywhere from zero to four (in the case of chorales,
more for arbitrary polyphonic scores) notes. Finally, the explicit
representation of tied notes vs articulated notes solves the problem plaguing
\citep{Eck2002}\citep{eck2008learning} \citep{Liu2014} \citep{Brien2016} where
multiple articulations at the same pitch are indistinguishable from a single
note with the same duration.

Additionally, notice that our encoding avoids hand-engineered features such as
pitch representations which are psychochologically-based \citep{mozer1994neural}
or harmonically-based \citep{franklin2004recurrent}
\citep{laden1989representation}. This is intentional and is motivated by
numerous reports \citep{bengio2009learning}\citep{Bengio2011} suggesting that
that a key ingredient in deep learning's success is its ability to learn good
features from raw data.

\section{Design and validation of a generative model for music}

In this section, we describe the design and validation process leading to our
generative model. Unlike many prior models for music data, we intentially avoid
injection of domain-specific knowledge into our model architectures such as
distinguishing between chords versus notes
\citep{hild1991harmonet}\citep{mozer1994neural} \citep{Eck2002} and explicitly
modelling of meter \citep{eck2008learning} or motifs
\citep{feulner1994melonet}. Through this fundamental connectionist approach, we
aim to minimize biases introduced by prior assumptions and force the model
itself to learn musical structure from data.

\subsection{Training and evaluation criteria}

Following \citep{mozer1994neural}, we will train the model to predict a distribution
distribution over all possible tokens next $\x_{t+1}$ given the current token
$\x_{t}$ and the previous hidden state $\h_{t-1}$. This is equivalent to
setting the target sequence to be the input sequence delayed by one timestep:
$\y_{1:T-1} = \x_{2:T}$ and $\y_T = \text{STOP}$. \mynote{Diagram for sequential prediction}.
\mynote{Note similarity with language modeling}.

For training criteria, we use the cross-entropy between the predicted
distributions $P(\y_t | \h_t, \x_t)$ and the actual target distribution
$\delta_{\y_t}$.

Note that our training criteria as written in \mynote{reference} uses the actual
next token $\x_{t+1}$ as the recurrent input, even if the most likely
prediction $\argmax P(\x_{t+1} | \h_t, \x_t)$ differs. This is is referred to
as \emph{teacher forcing}\citep{williams1989learning} and is motivated by the
observation that model predictions may not yet be correct during the early
iterations of training. However, at inference the token generated from
$P(\x_{t+1} | \h_t, \x_t)$ is reused as the previous input, creating a
discrepancy between training and inference. Scheduled sampling
\citep{bengio2015scheduled} is a recently proposed alternative training method
for resolving this discrepancy and may help the model better learn to predict
using generated symbols rather than relying on ground truth to be always
provided as input.

\subsection{Comparing memory cells for music data}

Using theanets

One-hot encoding, 64 dimensional vector embedding, RNN layer, fully connected layer, softmax.
\mynote{Diagram of the model}

In \cref{fig:theanets-architecture}, we compare various RNN architectures on
our data. All models utilized a RNN with \texttt{num\_layers=1},
\texttt{rnn\_size=130}, \texttt{wordvec=64} and differed only in memory cell
implementation. The clockwork RNN periods were set to $(1, 2, 4, 8, 16)$.

\begin{figure}[tb]
    \centering
    \input{Chapter4/Figs/theanets-architecture.pgf}
    \caption{LSTMs and GRUs yield the lowest training loss. Validation loss
      traces show all architectures exhibit signs of significant overfitting}
    \label{fig:theanets-architecture}
\end{figure}

The LSTM and GRU architectures achieve the lowest training errors, consistent
with expectations since these architectures have the most parameters. All
yielded comparable validation loss which increased over time, motivating
regularization.

LSTMs and GRUs trained much faster and achieved lower training loss, suggesting
higher capacity. \citep{Nayebi2015} reports that LSTMs outperform GRUs in music
applications, motivating our final choice for GRUs.

\subsection{Optimizing the LSTM architecture}
\label{sec:lstm-grid-search}

Switched to \texttt{torch-rnn}. \mynote{Discrepancy between above architectures
and below losses because we are perturbing about best model}

We construct multi-layer LSTM models with \texttt{num\_layers} number of
layers, each containing \texttt{rnn\_size} hidden units. The inputs $x_t$ are
one-hot-encoded before being passed through a \texttt{wordvec}-dimensional
vector-space embedding layer, which compresses the dimensionality down from
$|V| \approx 140$ to $\texttt{wordvec}$ dimensions. Dropout layers were added
between LSTM connections in both depth and time dimensions all with dropout
probability $\texttt{dropout} \in [0,1]$.

We build our models using the \texttt{torch7} framework and
an optimized implementation of LSTMs provided by \texttt{torch-rnn} \mynote{citep}.

Models were trained using RMSProp \mynote{citep} with batch normalization \mynote{citep}
and an initial learning rate of $2 \times 10^{-3}$ decayed by $0.5$ every $5$
epochs. The back-propogation through time gradients were clipped
at $t$ \mynote{citep Mikolov} and truncated after \texttt{seq\_length} time steps.
We use a mini-batch size of $50$.

\subsubsection{Overall best model}

We identified our best model to be \mynote{what is it?}.

\begin{figure}[tb]
  \centering
  \input{Chapter4/Figs/torch-rnn-best-model-trace.pgf}
  \caption{Training curves for the overall best model. The periodic spikes correspond
  to resetting of the LSTM state at the end of a training epoch.}
  \label{fig:torch-rnn-best-model-trace}
\end{figure}

In the following sections, we investigate pertubations about this model and the
effects of various hyperparameters. A complete listing of results are available
in \cref{tab:torch-rnn-config-perfs}.

\subsubsection{Regularization}

The increasing validation error in \cref{fig:theanets-architecture} confirmed
that our models were overfitting and required regularization. \texttt{dropout}

\begin{figure}[tb]
  \centering
  \input{Chapter4/Figs/torch-rnn-dropout.pgf}
  \caption{Dropout acts as a regularizer, resulting in larger training loss
  but better generalization as evidenced by lower validation loss. A setting of
\texttt{dropout=0.3} achieves best results for our model.}
  \label{fig:torch-rnn-dropout}
\end{figure}

\mynote{Batch normalization experiments}

\subsubsection{Network capacity}

\begin{figure}[tb]
    \centering
    \input{Chapter4/Figs/torch-rnn-network-params.pgf}
    \caption{\texttt{rnn\_size=256} and \texttt{num\_layers=3} yields lowest validation loss.}
    \label{fig:torch-rnn-network-params}
\end{figure}

\begin{figure}[tb]
  \centering
  \input{Chapter4/Figs/torch-rnn-network-params-num-layers.pgf}
  \caption{Validation loss improves initially with increasing network depth but deteriorates after $>3$ layers.}
  \label{fig:torch-rnn-network-params-num-layers}
\end{figure}

\begin{figure}[tb]
  \centering
  \input{Chapter4/Figs/torch-rnn-network-params-rnn-size.pgf}
  \caption{Validation loss improves initially with higher-dimensional hidden states
  but deteriorates after $>256$ dimensions.}
  \label{fig:torch-rnn-network-params-rnn-size}
\end{figure}

Sensitivity to network structure: \texttt{num\_layers} and \texttt{rnn\_size}.
\begin{itemize}
    \item Larger \texttt{rnn\_size} leads to higher capacity and lower training loss
        \begin{itemize}
            \item Presents as overfitting on validation, where the lowest capacity
                model \texttt{rnn\_size} appears to be improving in generalization while
                others are flat/increasing
        \end{itemize}
    \item Training curves about the same wrt \texttt{num\_layers}, validation curves have interesting story
        \begin{itemize}
            \item Depth matters: small 64 and 128 hidden unit RNNs saw improvements up to 0.09
            \item Expressivity gained from depth furthers overfitting: 256
                hidden unit RNN has some of the best validation performance at
                depth 1 but is the worst generalizing model for depths 2
                and 3 even though training loss is low
        \end{itemize}
    \item \texttt{rnn\_size=128} undisputably best generalizing, optimized at
        \texttt{num\_layers=2}: will continue with these settings
\end{itemize}

\subsubsection{Network input parameters}

\mynote{Is seq\_length an input parameter, or the BPTT parameters?}

\begin{figure}[tb]
    \centering
    \input{Chapter4/Figs/torch-rnn-input-params.pgf}
    \caption{\texttt{seq\_length=128} and \texttt{wordvec=32} yields lowest validation loss.}
    \label{fig:torch-rnn-input-params}
\end{figure}

\begin{figure}[tb]
  \centering
  \input{Chapter4/Figs/torch-rnn-input-params-wordvec.pgf}
  \caption{Pertubations about \texttt{wordvec=32} do not yield significant improvements.}
  \label{fig:torch-rnn-input-params-wordvec}
\end{figure}

Sensitivity to network inputs: \texttt{seq\_length} and \texttt{wordvec}
\begin{itemize}
    \item Training losses are about the same across all \texttt{wordvec}s
    \item Validation losses suggest that increasing \texttt{seq\_length} important for good performance \mynote{investigate further}
    \item \texttt{wordvec=128} overfits for all cases, the other two depend on \texttt{seq\_length} and vary an order of magnitude smaller than the performance gains from increasing \texttt{seq\_length}
\end{itemize}

\section{Results}

\citet{Allan2005} achieve cross-entropy losses of $2.79-2.80$ on unseen
test-set scores respectively for their ``harmonic skeleton'' subtask. This task
involves predictiong a sequence of one of 81 harmonic symbols, which may be
interpreted as equivalence classes of chords. Even after applying Viterbi
decoding to find globally optimal sequences, their HMM models achieve cross
entropies of $0.84-0.87$ on training-set scores (Table 5.2 in
\citet{Allan2005}).

The baseline results obtained by $N$-grams are shown in
\vref{tab:baseline-perplexity}. We compare against two widely available
language modelling software packages, KenLM \citep{Heafield-estimate} and
SRILM \citep{stolcke2002srilm}. KenLM implements an efficient moddified
Kneser-Ney smoothing language model and while SRILM provides a variety of
language models we choose choose to use the Good-Turing discounted language
model for benchmarking against.

In contrast, our best model achieves cross-entropy losses of $0.323$ on
training data and $0.477$ on held-out test data
(\vref{tab:torch-rnn-config-perfs}), corresponding to a training perplexity of
$1.251$ bits and a test perplexity of $1.391$. This is more
than $0.6$ bits lower than any test perplexity obtained by the $N$-gram
models compared in \href{tab:baseline-perplexity}, providing evidence
that the additional modelling capacity provided by RNNs is useful
for encoded music score data.

The best models investigated in \citet{Boulanger-Lewandowski2012} achieved
$-5.56$ log likelihood and $33.12\%$ accuracy on the symbolic prediction task.
Note that the log likelihood is significantly lower than the $-0.323$ log
likelihood implied by our model's cross-entropy loss. This is because the two
results are not comparable as they utilize different input encodings with
varying vocabulary sizes.

\mynote{Compare \citep{Brien2016}}.

\begin{table}[p]
  \centering
  \caption{Perplexities of baseline $N$-gram language models}
  \label{tab:baseline-perplexity}
\begin{tabular}{l c c c c}
  \toprule
  \multirow{2}{*}{Model Order} & \multicolumn{2}{c}{KenLM (Modified Kneser-Ney)} & \multicolumn{2}{c}{SRILM(Good-Turing)} \\
  \cmidrule{2-3}
  & Train & Test & Train & Test\\
  \midrule
1  & n/a   & n/a   & 34.84 & 34.807\\
2  & 9.376 & 8.245 & 9.420 & 9.334 \\
3  & 6.086 & 5.717 & 6.183 & 6.451 \\
4  & 3.865 & 4.091 & 4.089 & 4.676 \\
5  & 2.581 & 3.170 & 2.966 & 3.732 \\
6  & 1.594 & 2.196 & 2.002 & 2.738 \\
7  & 1.439 & 2.032 & 1.933 & 2.617 \\
8  & 1.387 & 2.014 & 1.965 & 2.647 \\
9  & 1.350 & 2.006 & 1.989 & 2.673 \\
10 & 1.323 & 2.001 & 1.569 & 2.591 \\
11 & 1.299 & 1.997 & 1.594 & 2.619 \\
12 & 1.284 & 2.000 & 1.633 & 2.664 \\
13 & 1.258 & 1.992 & 1.653 & 2.691 \\
14 & 1.241 & 1.991 & 1.682 & 2.730 \\
15 & 1.226 & 1.991 & 1.714 & 2.767 \\
16 & 1.214 & 1.994 & 1.749 & 2.807 \\
17 & 1.205 & 1.995 & 1.794 & 2.853 \\
18 & 1.196 & 1.993 & 1.845 & 2.901 \\
19 & 1.190 & 1.996 & 1.892 & 2.947 \\
20 & 1.184 & 1.997 & 1.940 & 2.990 \\
21 & 1.177 & 1.996 & 1.982 & 3.027 \\
22 & 1.173 & 1.997 & 2.031 & 3.067 \\
23 & 1.165 & 1.997 & 2.069 & 3.101 \\
24 & 1.159 & 1.998 & 2.111 & 3.135 \\
25 & 1.155 & 2.000 & 2.156 & 3.170 \\
  \bottomrule
\end{tabular}
\end{table}

\mynote{Compare on pitch/pitch class usage, note durations, meter, lengths of compositions}

\nomenclature[z-CPU]{CPU}{Central Processing Unit}
\nomenclature[z-GPU]{GPU}{Graphics Processing Unit}

\mynote{Show speedup when training with multi-GPU, selling point is how fast the model trains}

\begin{table}[tb]
  \centering
  \caption{Timing results comparing training on CPUs vs GPUs}
  \label{tab:label}
  \begin{tabular}{l c c c}
    \toprule
    \multirow{2}{*}{} & \multicolumn{2}{c}{Single Batch (seconds)} & 100 Epochs (seconds) \\
    \cmidrule{2-4}
    & mean & std & \\
    \midrule
    CPU & 4.287 & 0.311 & ??  \\
    GPU & 0.513 & 0.001 & 5614 \\
    \bottomrule
  \end{tabular}
\end{table}
\mynote{Fill in CPU total when converged}

\section{Other applications}

Scoring things as ``Bach-like'', model for expectation by using the probability.
