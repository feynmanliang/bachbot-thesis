\chapter{Automatic stylistic composition with deep LSTM}\label{ch:automatic-composition}
\begin{savequote}[75mm]
  We find ourselves in front of an attempt, as objective as possible, of creating
  an automated art, without any human interference except at the start, only in
  order to give the initial impulse and a few premises, like in the case
  of\ldots nothingness in the Big Bang Theory
  \qauthor{\citet{hoffmann2002towards}}
\end{savequote}
% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter4/Figs/Raster/}{Chapter4/Figs/PDF/}{Chapter4/Figs/}}
\else
    \graphicspath{{Chapter4/Figs/Vector/}{Chapter4/Figs/}}
\fi

This chapter describes the design and quantitative evaluation of a generative
RNN sequence model for polyphonic music. In contrast to many prior systems for
automatic composition, we intentionally avoid allowing our prior assumptions
about music theory and structure impact the design of our model, opting to
learn features from data over injecting prior knowledge. This choice is motivated
by three considerations:
\begin{enumerate}
  \item Prior assumptions about music may be incorrect, limiting the performance achievable by the model
  \item The goal is to assess the model's ability to compose convincing music, not the researcher's prior knowledge
  \item The structure learned by an assumption-free model may provide novel insights into various musical phenomena
\end{enumerate}
Note that this is deviates from many prior works, which leveraged
domain-specific knowledge such as modelling chords and notes hierarchically
\citep{hild1991harmonet,mozer1994neural,Eck2002}, accounting for meter
\citep{eck2008learning}, and detecting for motifs \citep{feulner1994melonet}.

We first construct a training corpus from Bach chorales and investigate the
impact of our preprocessing procedure on the corpus. Next, we present a simple
frame-based sequence encoding for polyphonic music with many desirable
properties. Using this sequence representation, we reduce the task to one of
language modelling and first show that traditional $N$-gram language models
perform poorly on our encoded music data. This prompts an investigation of
various RNN architectures, design trade-offs, and training methods in order to
build an optimized generative model for Bach chorales.
We conclude this chapter by quantitatively evaluating our final model in
test-set loss and training time, and comparing against similar work to
establish context.

\section{Constructing a corpus of encoded Bach chorales scores}

\nomenclature[z-BWV]{BWV}{Bach-Werke-Verzeichnis numbering system for Bach chorales}

We restrict the scope of our investigation to Bach chorales for the following reasons:
\begin{enumerate}
  \item The Baroque style employed in Bach chorales has specific guidelines and
    practices \citep{piston1978harmony} (\eg no parallel fifths, voice leading)
    which can be use to qualitatively evaluate success
  \item The large amount of easily recognizable structure: all chorales have
    exactly four parts consisting of a melody in the Soprano part harmonized by
    the Alto, Tenor, and Bass parts. Additionally, each chorale consists of a
    series of \emph{phrases}: ``groupings of consecutive notes into a unit that
    has complete musical sense of its own'''\citep{nattiez1990music} which Bach
    delimited using fermatas
  \item The Bach chorales have become a standardized corpus routinely studied
    by aspiring music theorists\citep{white2002guidelines}
\end{enumerate}

While the \textit{JCB Chorales} \citep{Allan2005} has become a popular dataset
for polyphonic music modelling, we will show in \ref{sec:jcb-distorts} that its quantization to quavers
introduces a non-negligible amount of distortortion.

Instead, we opt build a corpus of Bach chorales which is quantized to
semiquavers rather than quavers, enabling our model to operate at a \emph{time
resolution at least $2\times$ better than all related work}.

Our data is obtained from the \emph{Bach-Werke-Verzeichnis} (BWV)
\citep{butt1999bach} indexed collection of the Bach chorales provided by the
\texttt{music21}\citep{Scott2015} Python library.

\subsection{Preprocessing}\label{sec:preprocessing}

\nomenclature[z-MIDI]{MIDI}{Musical Instrument Device Interface}

Motivated by music's transposition invariance (see
\vref{sec:transposition-invariance}) as well as prior practice
\citep{mozer1994neural,Eck2002,franklin2004recurrent,franklin2005jazz}, we
first perform \emph{key normalization}. The keys of each score were first
analyzed using the Krumhansl Schmuckler key-finding algorithm
\citep{krumhansl2001cognitive} and then transposed such that the resulting
score is C-major for major scores and A-minor for minor scores.

Next, \emph{time quantization} is performed by aligning note start and end
times to the nearest multiple of some fundamental duration. Our model uses a
fundamental duration of one semibreve, \emph{exceeding the time resolutions of
\citep{Boulanger-Lewandowski2012,Eck2002} by 2x, \citep{hild1991harmonet} by
4x, and \citep{bellgard1994harmonizing} by 8x}.

We consider only note pitches and durations, neglecting changes in timing (\eg
ritardandos), dynamics (\eg crescendos), and additional notation (\eg accents,
staccatos, legatos). This is comparable to prior work
\citep{Boulanger-Lewandowski2012,pascanu2013construct} where a MIDI-encoding
also lacking this additional notation was used.

An example of the effects introduced by our preprocessing is provided in
\vref{fig:score-effects-preproc} in sheet music notation and in piano roll
notation on \vref{fig:piano-roll-effects-preproc}.
\begin{figure}[tb]
    \centering
    \includegraphics[width=0.8\linewidth]{bwv185-6-original-score-1.png}

    \vspace{1.5cm}

    \includegraphics[width=0.8\linewidth]{bwv185-6-preproc-score-1.png}
    \caption{First 4 bars of JCB Chorale BWV 185.6 before (top) and after
      (bottom) preprocessing. Note the transposition down by a semitone to
      C-major as well as quantization of the demisemiquavers in the third bar of
    the Soprano part.}
    \label{fig:score-effects-preproc}
\end{figure}

\mynote{Fix \vref{fig:piano-roll-effects-preproc} y-labels}

\begin{figure}[tb]
    \centering
        \input{Chapter4/Figs/bwv185-6-original-piano-roll.pgf}
        \input{Chapter4/Figs/bwv185-6-preproc-piano-roll.pgf}
    \caption{Piano roll representation of the same 4 bars from \cref{fig:score-effects-preproc}
      before and after preprocessing. Again, note the transposition to C-major
      and time-quantization occuring in the Soprano part.}
    \label{fig:piano-roll-effects-preproc}
\end{figure}

\subsubsection{Quantizing to semiquavers introduces non-negligible distortion}\label{sec:jcb-distorts}

Choosing to implement our own sequential encoding scheme was a difficult choice.
While it would permit a finer time-resolution of semiquavers, it would make
our cross-entropy losses incomparable to those reported on \textit{JCB
Chorales} \citep{Allan2005}.

To justify our decision, we investigated the distortion introduced by
quantization to quavers rather than semiquavers in
\vref{fig:note-lengths-time-quantization}. We find that \textit{JCB Chorales}
\emph{distorts $2816$ notes in the corpus ($2.85\%$) because of quantization to
quavers}. Since our research aim is to generate convincing music, we
\emph{minimize unnecessary distortions and proceed with our own encoding
scheme}, understanding that it will create difficulties in comparing
quantitative evaluation.

\begin{figure}[tb]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \input{Chapter4/Figs/note-lengths-original.pgf}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \input{Chapter4/Figs/note-lengths-quantized.pgf}
    \end{subfigure}
    \caption{Distortion introduced by quantization to semiquavers}
    \label{fig:note-lengths-time-quantization}
\end{figure}

We also investigate changes in other corpus-level statistics as a result of key
normalization and time quantization, such as pitch and pitch class usages and
meter. All results fall within expectations, but the interested reader is
directed to \vref{sec:quantify-effects-preprocessing}.

\subsection{Sequential encoding of musical data}
\label{sec:sequential-encoding}

\nomenclature[z-OOV]{OOV}{out of vocabulary}

After preprocessing of the scores, our next step is to encode music into a
sequence of tokens amenable for processing by RNNs. One design decision is
whether the tokens in the sequence are comprised of individual notes (as done
in \citep{mozer1994neural,franklin2004recurrent,sturm2016music}) or larger
harmonic units (\eg chords \citep{Eck2002,Boulanger-Lewandowski2012},
``harmonic context'' \citep{Allan2005}). This tradeoff is similar to one faced
in RNN language modelling where either individual characters or entire words
can be used.

In contrast to most language models which operate at the word level, we choose
to construct our models at the note level. The use of \emph{a note-level encoding may
improve performance with respect to out-of-vocabulary (OOV) tokens} in two ways.
It first reduces the potential vocabulary size from $O(128^4)$ possible chords
down to $O(128)$ potential notes. In addition, harmonic relationships learned
by the model parameters may enable generalization to OOV queries (\eg OOV
chords that are transpositions of in-vocabualry chords).

In fact, the decision may not even matter at all. \citet{graves2013generating}
showed comparable performance between LSTM language models that operate on
individual characters versus words (perplexities of $1.24$ bits vs $1.23$ bits
per character respectively), suggesting that choice of notes versus chords is not
very significant, at least for English language modelling.

Similar to \citep{todd1989connectionist}, our encoding represents polyphonic
scores using a localist frame-based representation where time is discretized
into constant timestep \emph{frames}. Frame based processing forces the network
to learn the relative duration of notes, a counting and timing task which
\citep{gers2002learning} demonstrated LSTM is capable of. Consecutive frames
are separated by a unique delimiter (``$|||$''' in
\vref{fig:eg-encoded-score}). Each frame consists of a sequence of $\langle
\text{Note}, \text{Tie} \rangle$ tuples where $\text{Note} \in
\{0,1,\cdots,127\}$ represents the MIDI pitch of a note and $\text{Tie} \in
\{True,False\}$ distinguishes whether a note is tied with a note at the same
pitch from the previous frame or is articulated at the current timestep.

\begin{figure}[p]
  \centering
  \begin{verbatim}
                              (59, True)
                              (56, True)
                              (52, True)
                              (47, True)
                              |||
                              (59, True)
                              (56, True)
                              (52, True)
                              (47, True)
                              |||
                              (59, True)
                              (56, True)
                              (52, True)
                              (47, True)
                              |||
                              (.)
                              (57, False)
                              (52, False)
                              (48, False)
                              (45, False)
                              |||
                              (.)
                              (57, True)
                              (52, True)
                              (48, True)
                              (45, True)
                              |||
  \end{verbatim}
  \caption{Example encoding of two chords, the second one annotated with fermatas (tokens $122$ to $148$, BWV 101.7).
  chords are encoded as (MIDI pitch value, tied to previous frame?) tuples,
  ``|||''' encodes the ends of frames, and ``(.)'' at the start of a chord
  encodes a fermata. Each ``|||'' corresponds to time advancing by a semiquaver}
  \label{fig:eg-encoded-score}
\end{figure}

A design decision is the order in which notes within a frame are encoded and
consequentially processed by a sequential model. Since chorale music places the
melody in the Soprano part, it is reasonable to expect the Soprano notes to be
most significant in determining the other parts. Hence, we would like to process
Soprano notes first and \emph{order the notes within a frame in descending pitch}.

The above specification describes our initial attempt at an encoding format.
However, we found that this encoding format resulted in unrealistically long
phrase lengths. Including fermatas (represented by ``(.)'' in
\vref{fig:eg-encoded-score}), which Bach used to denote ends of phrases, solves
this problem.

Finally, for each score a unique start symbol (``START'' in \cref{fig:eg-encoded-score})
and end symbol (``END'' in \cref{fig:eg-encoded-score}) are appended to the beginning and
end respectively. This causes the model to learn to initialize itself when
given the start symbol and allows us to determine when a composition generated
by the model has concluded.

The vocabulary and corpus size after encoding is detailed in
\cref{tab:encoded-corpus-stats}. The rank-size distribution of the note-level
corpus tokens is shown in \cref{fig:zipf} and confirms the failure of Zipf's
law in our data.

\begin{table}[tb]
  \centering
  \caption{Statistics on the preprocessed datasets used throughout our study}
  \label{tab:encoded-corpus-stats}.
  \begin{tabular}{c c c c}
    \toprule
    Vocabulary size & Total \# tokens & Training size & Validation size \\
    \midrule
    108 & 423463 & 381117 & 42346 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[tb]
  \centering
  \input{Chapter4/Figs/zipf-law-note-tuples.pgf}
  \caption{Left: Token frequencies sorted by rank. Right: log-log plot where
  a power law distribution as predicted by Zipf's law would appear linear.}
  \label{fig:zipf}
\end{figure}

We make the following observations about our proposed encoding scheme:
\begin{itemize}
  \item It is \emph{sparse}: unarticulated notes are not encoded
  \item It is also \emph{variable length}: each frame can span anywhere from one to five tokens, requiring
    LSTM's capability of detecting spacing between events\citep{gers2002learning}
  \item The explicit representation of tied notes vs articulated notes \emph{enables us to
    determine when notes end}, resolving an issue present in many prior works
    \citep{Eck2002,eck2008learning,Liu2014,Brien2016}
\end{itemize}

Unlike many others
\citep{mozer1994neural,franklin2004recurrent,laden1989representation}, we avoid
adding prior informaton through engineering harmonically relevant features.
Instead, we appeal to results by \citet{bengio2009learning,Bengio2011}
suggesting that that a key ingredient in deep learning's success is its \emph{ability
to learn good features from raw data}. Such features are very likely to be
musically relevant, which we will explore further in \cref{sec:model-analysis}.

\section{Design and validation of a generative model for music}

In this section, we describe the design and validation process leading to our
generative model. 

\subsection{Training and evaluation criteria}

Following \citep{mozer1994neural}, we will train the model to predict
$P(\x_{t+1} | \x_t, \h_{t-1})$: a probability distribution over all possible
next tokens $\x_{t+1}$ given the current token $\x_{t}$ and the previous hidden
state $\h_{t-1}$. This is the exact same operation performed by RNN language
models \citep{Mikolov2010}. We minimize cross-entropy loss between the
predicted distributions $P(\x_{t+1} | \x_t, \h_{t-1})$ and the actual target
distribution $\delta_{\x_{t+1}}$

At the next timestep, the correct token $\x_{t+1}$ is provided as the recurrent
input even if the most likely prediction $\argmax P(\x_{t+1} | \h_t, \x_t)$
differs. This is is referred to as \emph{teacher forcing}
\citep{williams1989learning} performed to aid convergence because the model's
predictions may not be reliable early in training.

However, at inference the token generated from $P(\x_{t+1} | \h_t, \x_t)$ is
reused as the previous input, creating a discrepancy between training and
inference. Scheduled sampling \citep{bengio2015scheduled} is a recently
proposed alternative training method for resolving this discrepancy and may
help the model better learn to predict using generated symbols rather than
relying on ground truth to be always provided as input.

\subsection{Establishing a baseline with $N$-gram language models}

The encoding of music scores into token sequences permits application of
standard sequence modelling techniques from \emph{language modelling}, a
research topic within speech recognition concerned with modelling distributions
over sequences of tokens (\eg phones, words). This motivates our use of two
widely available language modelling software packages, KenLM
\citep{Heafield-estimate} and SRILM \citep{stolcke2002srilm}, as baselines.
KenLM implements an efficient modified Kneser-Ney smoothing language model and
while SRILM provides a variety of language models we choose choose to use the
Good-Turing discounted language model for benchmarking against.

Both models were developed for applications modelling language data, whose distribution
over words which may differ from our encoded music data (see \vref{fig:zipf}).
Furthermore, both are based upon $N$-gram models which are constrained to only
account for short-term dependencies. Therefore, we expect RNNs to outperform the
$N$-gram baselines shown in \vref{tab:baseline-perplexity}.

\begin{table}[p]
  \centering
  \caption{Perplexities of baseline $N$-gram language models on encoded music data}
  \label{tab:baseline-perplexity}
\begin{tabular}{l c c c c}
  \toprule
  \multirow{2}{*}{Model Order} & \multicolumn{2}{c}{KenLM (Modified Kneser-Ney)} & \multicolumn{2}{c}{SRILM(Good-Turing)} \\
  \cmidrule{2-3}
  & Train & Test & Train & Test\\
  \midrule
1  & n/a   & n/a   & 34.84 & 34.807\\
2  & 9.376 & 8.245 & 9.420 & 9.334 \\
3  & 6.086 & 5.717 & 6.183 & 6.451 \\
4  & 3.865 & 4.091 & 4.089 & 4.676 \\
5  & 2.581 & 3.170 & 2.966 & 3.732 \\
6  & 1.594 & 2.196 & 2.002 & 2.738 \\
7  & 1.439 & 2.032 & 1.933 & 2.617 \\
8  & 1.387 & 2.014 & 1.965 & 2.647 \\
9  & 1.350 & 2.006 & 1.989 & 2.673 \\
10 & 1.323 & 2.001 & 1.569 & 2.591 \\
11 & 1.299 & 1.997 & 1.594 & 2.619 \\
12 & 1.284 & 2.000 & 1.633 & 2.664 \\
13 & 1.258 & 1.992 & 1.653 & 2.691 \\
14 & 1.241 & 1.991 & 1.682 & 2.730 \\
15 & 1.226 & 1.991 & 1.714 & 2.767 \\
16 & 1.214 & 1.994 & 1.749 & 2.807 \\
17 & 1.205 & 1.995 & 1.794 & 2.853 \\
18 & 1.196 & 1.993 & 1.845 & 2.901 \\
19 & 1.190 & 1.996 & 1.892 & 2.947 \\
20 & 1.184 & 1.997 & 1.940 & 2.990 \\
21 & 1.177 & 1.996 & 1.982 & 3.027 \\
22 & 1.173 & 1.997 & 2.031 & 3.067 \\
23 & 1.165 & 1.997 & 2.069 & 3.101 \\
24 & 1.159 & 1.998 & 2.111 & 3.135 \\
25 & 1.155 & 2.000 & 2.156 & 3.170 \\
  \bottomrule
\end{tabular}
\end{table}

\subsection{Description of RNN model hyperparameters}

The following experiments investigate deep RNN models parameterized by the
following hyperparameters:
\begin{enumerate}
  \item \texttt{num\_layers} -- the number of memory cell layers
  \item \texttt{rnn\_size} -- the number of hidden units per memory cell (\ie hidden state dimension)
  \item \texttt{wordvec} -- dimension of vector embeddings
  \item \texttt{seq\_length} -- number of frames before truncating BPTT gradient
  \item \texttt{dropout} -- the dropout probability
\end{enumerate}

\mynote{Does this need to be diagrammed?}

Our model first embeds the inputs $\x_t$ into a \texttt{wordvec}-dimensional
vector-space, compressing the dimensionality down from $|V| \approx 140$ to
$\texttt{wordvec}$ dimensions. Next, \texttt{num\_layers} layers of memory
cells followed by batch normalization \citep{ioffe2015batch} and dropout
\citep{hinton2012improving} with dropout probability \texttt{dropout} are
stacked. The outputs $\y^{(\texttt{num\_layers})}_t$ are followed by a
fully-connected layer mapping to $|V| = 108$ units, which are passed through a
softmax to yield a predictive distribution $P(\x_{t+1} | \h_{t-1}, \x_{t})$.
Cross entropy is used as the loss minimized during training.

Models were trained using Adam \citep{kingma2014adam} with an initial learning
rate of $2 \times 10^{-3}$ decayed by $0.5$ every $5$ epochs. The
back-propogation through time gradients were clipped at $\pm5.0$
\citep{Pascanu2012} and BPTT was truncated after \texttt{seq\_length} frames. A
minibatch size of $50$ was used.

\subsection{Comparison of memory cells on music data}

We used \texttt{theanets}\footnote{https://github.com/lmjohns3/theanets} to
rapidly implement and compare a large number of memory cell implementations.
\Cref{fig:theanets-architecture} shows the results of exploring a range of RNN
memory cell implementation and holding \texttt{num\_layers=1},
\texttt{rnn\_size=130}, \texttt{wordvec=64}, and \texttt{seq\_length=50}
constant. Unlike later models, none of these models utilized dropout or batch
normalization. We configured the clockwork RNN \citep{cho2014learning} with $5$
equal-sized hidden state blocks with update periods $(1, 2, 4, 8, 16)$.

\begin{figure}[tb]
    \centering
    \input{Chapter4/Figs/theanets-architecture.pgf}
    \caption{LSTM and GRUs yield the lowest training loss. Validation loss
      traces show all architectures exhibit signs of significant overfitting}
    \label{fig:theanets-architecture}
\end{figure}

\Cref{fig:theanets-architecture} shows that while all models achieved similar
validation losses, LSTM and GRUs trained much faster and achieved lower
training loss. Since \citet{zaremba2015empirical} find similar empirical
performance between LSTM and GRUs and \citet{Nayebi2015} observe LSTM
outperforming GRUs in music applications, we choose to use LSTM as the memory
cell for all following experiments.

The increasing validation loss over time in \cref{fig:theanets-architecture}
is a red flag suggesting that overfitting is occuring. This observation motivates
the exporation of dropout regularization in \cref{sec:lstm-dropout}.

\subsection{Optimizing the LSTM architecture}\label{sec:lstm-grid-search}
\nomenclature[z-CPU]{CPU}{Central Processing Unit}
\nomenclature[z-GPU]{GPU}{Graphics Processing Unit}

After settling on LSTM as the memory cell, we conducted remaining experiments
using the \texttt{torch-rnn} Lua software library. Our switch was motivated by
support for GPU training (see \vref{tab:gpu-training}), dropout, and batch normalization.

\subsubsection{Dropout regularization improves validation loss}\label{sec:lstm-dropout}

The increasing validation errors in \vref{fig:theanets-architecture} prompted
investigation of regularization techniques. In addition to adding batch
normalization, a technique known to reduce overfitting and accelerate training
\cite{ioffe2015batch}, we also investigated the effects of different levels
of dropout by varying the \texttt{dropout} parameter.

\begin{figure}[tb]
  \centering
  \input{Chapter4/Figs/torch-rnn-dropout.pgf}
  \caption{Dropout acts as a regularizer, resulting in larger training loss
  but better generalization as evidenced by lower validation loss. A setting of
\texttt{dropout=0.3} achieves best results for our model.}
  \label{fig:torch-rnn-dropout}
\end{figure}

The experimental results are shown in \cref{fig:torch-rnn-dropout}. As
expected, dropout acts as a regularizer and reduces validation loss from $0.65$
down to $0.477$ (when \texttt{dropout=0.3}). Training loss has slightly
increased, which is also unexpected as application of dropout during training
introduces additional noise into the model.

\subsubsection{Overall best model}\label{sec:overall-best-model}

We perform a grid search through the following parameter grid:
\begin{itemize}
  \item \texttt{num\_layers} $\in \{1,2,3,4\}$
  \item \texttt{rnn\_size} $\in \{128, 256, 384, 512\}$
  \item \texttt{wordvec} $\in \{16, 32, 64\}$
  \item \texttt{seq\_length} $\in \{64,128,256\}$
  \item \texttt{dropout} $\{0.0, 0.1, 0.2, 0.3, 0.4, 0.5 \}$
\end{itemize}
A full listing of results is provided in \vref{tab:torch-rnn-config-perfs}.

The optimal hyperparameter settings within our grid was found to be
$\texttt{num\_layers}=3$, $\texttt{rnn\_size}=$, $\texttt{wordvec}=32$,
$\texttt{seq\_length}=128$ $\texttt{dropout}=0.3$. Such a model achieves
$0.324$ and $0.477$ cross entropy losses on training and validation corpuses
respectively. \Cref{fig:torch-rnn-best-model-trace} plots the training curve of
this model and shows that \emph{training converges after only 30 iterations ($\approx
28.5$ minutes on a single GPU)}.

\begin{figure}[tb]
  \centering
  \input{Chapter4/Figs/torch-rnn-best-model-trace.pgf}
  \caption{Training curves for the overall best model. The periodic spikes correspond
  to resetting of the LSTM state at the end of a training epoch.}
  \label{fig:torch-rnn-best-model-trace}
\end{figure}

To confirm local optimality, we perform perturbations about our final
hyperparameter settings in
\crefrange{fig:torch-rnn-network-params}{fig:torch-rnn-input-params-wordvec}.
Our analysis of these experiments yield the following insights:
\begin{enumerate}
  \item Depth matters! Increasing \texttt{num\_layers} can yield up to $9\%$ lower validation loss.
    The best model is $3$ layers deep, any further and overfitting occurs.
    This finding is unsurprising: the dominance of deep RNNs in polyphonic
    modelling was already noted by \citet{pascanu2013construct}
  \item Increasing hidden state size (\texttt{rnn\_size}) improves model capacity, but causing overfitting when too large
  \item The exact size of the vector embeddings (\texttt{wordvec}) did not appear significant
  \item While training losses did not change, increasing the BPTT truncation length (\texttt{seq\_length})
    decreased validation loss, suggesting improved generalization
\end{enumerate}

\subsection{GPU training yields $800\%$ acceperation}

Consistent with prior work \citep{sutskever2014sequence,kaiser2015neural},
timing results \cref{tab:gpu-training} from training our overall best model
confirmed a $800\%$ speedup enabled by the GPU training implemented in
\texttt{torch-rnn}.

\begin{table}[tb]
  \centering
  \caption{Timing results comparing CPU and GPU training of the overall best model (\vref{sec:overall-best-model})}
  \label{tab:gpu-training}
  \begin{tabular}{l c c c}
    \toprule
    \multirow{2}{*}{} & \multicolumn{2}{c}{Single Batch} & 30 Epochs (seconds) \\
    \cmidrule{2-4}
    & mean (sec) & std (sec) & (minutes)\\
    \midrule
    CPU & 4.287 & 0.311 & 256.8\\
    GPU & 0.513 & 0.001 &  28.5\\
    \bottomrule
  \end{tabular}
\end{table}

\section{Results and comparison}

As done by \citep{bayer2013fast,Boulanger-Lewandowski2012}, we quantitatively
evaluate our models using cross entropies and perplexities on a 10\% held-out
validation set. Our best model (\vref{tab:torch-rnn-config-perfs}) achieves
\emph{cross-entropy losses of $0.323$ on training data and $0.477$ on held-out test
data, corresponding to a training perplexity of $1.251$ bits and a test
perplexity of $1.391$}. As expected, the deep LSTM model achieves more than
\emph{$0.6$ bits lower than any validation perplexity obtained by the $N$-gram
models} compared in \vref{tab:baseline-perplexity}.

% Before presenting the following comparisons, make note that cross-entropy loss
% results are not directly comparable. This is because our music encoding scheme
% (described in \vref{sec:sequential-encoding}) differs from the \textit{JCB
% Chorales} \citep{Allan2005} dataset used by other models.

% On the \textit{(JCB Chorales} dataset \citet{Allan2005} report cross-entropy
% losses of $2.79-2.80$  for sequential prediction of ``harmonic skeletons,''
% which dropped to $0.84-0.87$ after applying Viterbi decoding (Table 5.2 in
% \citet{Allan2005}). The RNN-RBM by \citet{Boulanger-Lewandowski2012} achieved
% $-5.56$ log likelihood and $33.12\%$ accuracy on the same dataset.
