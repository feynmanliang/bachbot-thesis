\chapter{Automatic composition with deep LSTM}\label{ch:automatic-composition}
\begin{savequote}[75mm]
  We find ourselves in front of an attempt, as objective as possible, of creating
  an automated art, without any human interference except at the start, only in
  order to give the initial impulse and a few premises, like in the case
  of\ldots nothingness in the Big Bang Theory
  \qauthor{\citet{hoffmann2002towards}}
\end{savequote}
% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter4/Figs/Raster/}{Chapter4/Figs/PDF/}{Chapter4/Figs/}}
\else
    \graphicspath{{Chapter4/Figs/Vector/}{Chapter4/Figs/}}
\fi

This chapter describes the design and quantitative evaluation of a generative
RNN sequence model for polyphonic music. We first construct a training corpus
from Bach chorales and investigate the impact of our preprocessing procedure on
the corpus. Next, we present a simple frame-based sequence encoding for
polyphonic music with many desirable properties.

Using this sequence representation, we reduce the task to one of language
modelling and first show that traditional $N$-gram language models perform poorly
on our encoded music data. This prompts an investigation of various RNN
architectures, design trade-offs, and training methods in order to build
an optimized generative model for Bach chorales.

We conclude this chapter by quantitatively evaluating our final model in
test-set loss and training time, and comparing against similar work to
establish context.

\section{Constructing a corpus of encoded Bach chorales scores}

\nomenclature[z-BWV]{BWV}{Bach-Werke-Verzeichnis numbering system for Bach chorales}

We restrict the scope of our investigation to Bach chorales for the following reasons:
\begin{enumerate}
  \item The Baroque style employed in Bach chorales has specific guidelines and
    practices \citep{piston1978harmony} (\eg no parallel fifths, voice leading)
    which can be use to qualitatively evaluate success
  \item The large amount of easily recognizable structure: all chorales have
    exactly four parts consisting of a melody in the Soprano part harmonized by
    the Alto, Tenor, and Bass parts. Additionally, each chorale consists of a
    series of \emph{phrases}: ``groupings of consecutive notes into a unit that
    has complete musical sense of its own'''\citep{nattiez1990music} which Bach
    delimited using fermatas
  \item The Bach chorales have become a standardized corpus routinely studied
    by aspiring music theorists\citep{white2002guidelines}
\end{enumerate}
We use the \emph{Bach-Werke-Verzeichnis} (BWV) \citep{butt1999bach} indexed
collection of the Bach chorales provided by the
\texttt{music21}\citep{Scott2015} Python library as our data source.

\subsection{Preprocessing}\label{sec:preprocessing}

\nomenclature[z-MIDI]{MIDI}{Musical Instrument Device Interface}

Motivated by transposition invariance in music (see
\vref{sec:transposition-invariance}) and prior work
\citep{mozer1994neural,Eck2002,franklin2004recurrent,franklin2005jazz}, we
first perform \emph{key normalization}. The keys of each score were first
analyzed using the Krumhansl Schmuckler key-finding algorithm
\citep{krumhansl2001cognitive} and then transposed such that the resulting
score is C-major for major scores and A-minor for minor scores.

Next, \emph{time quantization} is performed by aligning note start and end
times to the nearest multiple of some minimum duration. Our model uses a
minimum duration of one semibreve, exceeding the time resolutions of
\citep{Boulanger-Lewandowski2012,Eck2002} by 2x,
\citep{hild1991harmonet} by 4x, and \citep{bellgard1994harmonizing} by 8x.

We consider only note pitches and durations, neglecting changes in timing (\eg
ritardandos), dynamics (\eg crescendos), and stylistic notations (\eg accents,
staccatos, legatos). This is comparable to prior work
\citep{Boulanger-Lewandowski2012,pascanu2013construct} where a MIDI-encoding
also lacking this additional notation was used.

An example of the distortion introduced through of our preprocessing steps is
provided in \vref{fig:score-effects-preproc} in sheet music notation and in
piano roll notation on \vref{fig:piano-roll-effects-preproc}.
\begin{figure}[tb]
    \centering
    \includegraphics[width=0.8\linewidth]{bwv185-6-original-score-1.png}

    \vspace{1.5cm}

    \includegraphics[width=0.8\linewidth]{bwv185-6-preproc-score-1.png}
    \caption{First 4 bars of JCB Chorale BWV 185.6 before (top) and after
      (bottom) preprocessing. Note the transposition down by a semitone to
      C-major as well as quantization of the demisemiquavers in the third bar of
    the Soprano part.}
    \label{fig:score-effects-preproc}
\end{figure}

\begin{figure}[tb]
    \centering
        \input{Chapter4/Figs/bwv185-6-original-piano-roll.pgf}
        \input{Chapter4/Figs/bwv185-6-preproc-piano-roll.pgf}
    \caption{Piano roll representation of the same 4 bars from \cref{fig:score-effects-preproc}
      before and after preprocessing. Again, note the transposition to C-major
      and time-quantization occuring in the Soprano part.}
    \label{fig:piano-roll-effects-preproc}
\end{figure}

\subsubsection{Corpus level analysis of preprocessing effects}

To assess the effects introduced by key normalization and time quantization,
we analyze corpus level statistics related to pitch and duration.

\mynote{Trim since we moved a lot to appendix}

\cref{fig:pitch-key-standardization} plots a histogram of pitch usage counts
before and after key normalization. Notice that the overall range of pitches
has increased after key normalization. This can be explained by noting that
Bach's chorales were to be performed by vocalists and hence were restricted to
use pitches within human voice ranges regardless of key. After transposition,
this constraint is no longer be satisfied and we see the appearance of
unrealistically low notes (\eg A1) outside the range of even the lowest voice
types.

In \cref{fig:pc-key-standardization}, we visualize histograms of pitch class usages.
As expected, key normalization has increased the usage of pitch classes in the key of
C-major / A-minor (\ie those which possess no accidentals) and decreased out of key
pitch classes (\eg C\#, F\#).

\mynote{Add tables to quantify, argue that semiquaver quantization is a big win for us}
\begin{figure}[tb]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \input{Chapter4/Figs/note-lengths-original.pgf}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \input{Chapter4/Figs/note-lengths-quantized.pgf}
    \end{subfigure}
    \caption{Distribution of note durations over Bach chorales corpus. Quantization has minimal impact
    because of the high resolution (semiquavers) used.}
    \label{fig:note-lengths-time-quantization}
\end{figure}

We investigate the effects of time quantization in
\cref{fig:note-lengths-time-quantization}, which shows histograms of note
duration usages before and after quantization. \mynote{Update plots... are they affected}.


\subsection{Sequential encoding of musical data}
\label{sec:sequential-encoding}

After preprocessing of the scores, our next step is to encode music into a
sequence of tokens amenable for processing by RNNs. One design decision is
whether the tokens in the sequence are comprised of individual notes (as done
in \citep{mozer1994neural,franklin2004recurrent,sturm2016music}) or larger
harmonic units (\eg chords \citep{Eck2002,Boulanger-Lewandowski2012},
``harmonic context'' \citep{Allan2005}). This tradeoff is similar to one faced
in RNN language modelling where either individual characters or entire words
can be used.

In contrast to most language models which operate at the word level, we choose
to construct our models at the note level for several reasons.
Firstly, the issue of multiple tokens in the sequence corresponding to the same
instant of time in the represented music should not be problematic because
LSTM have been shown to be able to learn to implement precise timing and
counting\cite{gers2002learning}. Additionally, the use of a note-level encoding
partially mitigates the problem of out-of-vocabulary (OOV) tokens in two ways.
Besides reducing the potential vocabulary size from $O(128^4)$ possible chords
to $O(128)$ potential notes, the model is now able to capture harmonic
relationships between notes within the LSTM model weights ($\W_{xx}$,
$\W_{xh}$, $\W_{hh}$ in \mynote{reference}) and may generalize better to unseen
chords. Furthermore, \citet{graves2013generating} showed comparable performance
between LSTM language models that operate on individual characters versus words
(perplexities of $1.24$ bits vs $1.23$ bits per character respectively),
suggesting that the choice is not too significant at least for English language
modelling.

Similar to \citep{todd1989connectionist}, we represent polyphonic scores using
a localist frame-based representation where time is discretized into constant
timestep \emph{frames}. Frame based processing forces the network to learn the
relative duration of notes, a counting and timing task which
\citep{gers2002learning} demonstrated LSTM is capable of. Consecutive frames are
separated by a unique delimiter (``$|||$''' in \mynote{Figure of score encoded
in text}).

\mynote{Add an example encoded score}

Each frame consists of a sequence of $\langle \text{Note}, \text{Tie} \rangle$
tuples where $\text{Note} \in \{0,1,\cdots,127\}$ represents the MIDI pitch of
a note and $\text{Tie} \in \{T,F\}$ distinguishes whether a note is tied with a
note at the same pitch from the previous frame or is articulated at the current
timestep. A design decision here is the order in which notes within a frame are
encoded and consequentially processed by a sequential model. Since chorale
music places the melody in the Soprano part, it is reasonable to expect the
Soprano notes to be most significant in determining the other parts. Hence, we
choose to process the Soprano notes first and order notes in descending pitch
within every frame.

The above specification describes our initial encoding. Later in our work
\mynote{reference}, we found that this encoding resulted in unrealistically long
phrase lengths. Including fermatas (represented by ``(.)'' in \mynote{Figure of
encoded score}, which Bach used to denote ends of phrases, solves this problem.

Finally, for each score a unique start symbol (``START'' in \mynote{Figure})
and end symbol (``END'' in \mynote{Figure}) are appended to the beginning and
end respectively. This causes the model to learn to initialize itself when
given the start symbol and allows us to determine when a composition generated
by the model has concluded. The vocabulary and corpus size after encoding is
detailed in \cref{tab:encoded-corpus-stats}. The rank-size distribution of the
note-level corpus tokens is shown in \cref{fig:zipf} and confirms the failure
of Zipf's law in our data.

\begin{table}[tb]
  \centering
  \caption{Statistics on the preprocessed datasets used throughout our study}
  \label{tab:encoded-corpus-stats}.
  \begin{tabular}{c c c c}
    \toprule
    Vocabulary size & Total \# tokens & Training size & Validation size \\
    \midrule
    108 & 423463 & 381117 & 42346 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[tb]
  \centering
  \input{Chapter4/Figs/zipf-law-note-tuples.pgf}
  \caption{Left: Token frequencies sorted by rank. Right: log-log plot where
  a power law distribution as predicted by Zipf's law would appear linear.}
  \label{fig:zipf}
\end{figure}

\mynote{Trim this stuff}
Notice that our encoding is sparse: unarticulated notes are not encoded. It is
also variable length as anywhere from zero to four (in the case of chorales,
more for arbitrary polyphonic scores) notes. Finally, the explicit
representation of tied notes vs articulated notes solves the problem where
multiple articulations at the same pitch are indistinguishable from a single
note with the same duration, an issue present in many prior works
\citep{Eck2002,eck2008learning,Liu2014,Brien2016}.

Additionally, notice that our encoding avoids hand-engineered features such as
pitch representations which are psychologically-based \citep{mozer1994neural}
or harmonically-based \citep{franklin2004recurrent}
\citep{laden1989representation}. This is intentional and is motivated by
numerous reports \citep{bengio2009learning}\citep{Bengio2011} suggesting that
that a key ingredient in deep learning's success is its ability to learn good
features from raw data.

\section{Design and validation of a generative model for music}

In this section, we describe the design and validation process leading to our
generative model. Unlike many prior models for music data, we intentionally avoid
injection of domain-specific knowledge into our model architectures such as
distinguishing between chords versus notes
\citep{hild1991harmonet}\citep{mozer1994neural} \citep{Eck2002} and explicitly
modelling of meter \citep{eck2008learning} or motifs
\citep{feulner1994melonet}. Through this fundamental connectionist approach, we
aim to minimize biases introduced by prior assumptions and force the model
itself to learn musical structure from data.

\subsection{Training and evaluation criteria}

Following \citep{mozer1994neural}, we will train the model to predict a distribution
distribution over all possible tokens next $\x_{t+1}$ given the current token
$\x_{t}$ and the previous hidden state $\h_{t-1}$. This is equivalent to
setting the target sequence to be the input sequence delayed by one timestep:
$\y_{1:T-1} = \x_{2:T}$ and $\y_T = \text{STOP}$. \mynote{Diagram for sequential prediction}.
\mynote{Note similarity with language modeling}.

For training criteria, we use the cross-entropy between the predicted
distributions $P(\y_t | \h_t, \x_t)$ and the actual target distribution
$\delta_{\y_t}$.

Note that our training criteria as written in \mynote{reference} uses the actual
next token $\x_{t+1}$ as the recurrent input, even if the most likely
prediction $\argmax P(\x_{t+1} | \h_t, \x_t)$ differs. This is is referred to
as \emph{teacher forcing}\citep{williams1989learning} and is motivated by the
observation that model predictions may not yet be correct during the early
iterations of training. However, at inference the token generated from
$P(\x_{t+1} | \h_t, \x_t)$ is reused as the previous input, creating a
discrepancy between training and inference. Scheduled sampling
\citep{bengio2015scheduled} is a recently proposed alternative training method
for resolving this discrepancy and may help the model better learn to predict
using generated symbols rather than relying on ground truth to be always
provided as input.

\subsection{Establishing a baseline with $N$-gram language models}

The encoding of music scores into token sequences permits application of
standard sequence modelling techniques from \emph{language modelling}, a
research topic within speech recognition concerned with modelling distributions
over sequences of tokens (\eg phones, words). This motivates our use of two
widely available language modelling software packages, KenLM
\citep{Heafield-estimate} and SRILM \citep{stolcke2002srilm}, as baselines.
KenLM implements an efficient modified Kneser-Ney smoothing language model and
while SRILM provides a variety of language models we choose choose to use the
Good-Turing discounted language model for benchmarking against.

Both models were developed for applications modelling language data, whose distribution
over words which may differ from our encoded music data (see \vref{fig:zipf}).
Furthermore, both are based upon $N$-gram models which are constrained to only
account for short-term dependencies. Hence, we expect RNNs to outperform the
results shown in \vref{tab:baseline-perplexity}.

\begin{table}[p]
  \centering
  \caption{Perplexities of baseline $N$-gram language models on encoded music data}
  \label{tab:baseline-perplexity}
\begin{tabular}{l c c c c}
  \toprule
  \multirow{2}{*}{Model Order} & \multicolumn{2}{c}{KenLM (Modified Kneser-Ney)} & \multicolumn{2}{c}{SRILM(Good-Turing)} \\
  \cmidrule{2-3}
  & Train & Test & Train & Test\\
  \midrule
1  & n/a   & n/a   & 34.84 & 34.807\\
2  & 9.376 & 8.245 & 9.420 & 9.334 \\
3  & 6.086 & 5.717 & 6.183 & 6.451 \\
4  & 3.865 & 4.091 & 4.089 & 4.676 \\
5  & 2.581 & 3.170 & 2.966 & 3.732 \\
6  & 1.594 & 2.196 & 2.002 & 2.738 \\
7  & 1.439 & 2.032 & 1.933 & 2.617 \\
8  & 1.387 & 2.014 & 1.965 & 2.647 \\
9  & 1.350 & 2.006 & 1.989 & 2.673 \\
10 & 1.323 & 2.001 & 1.569 & 2.591 \\
11 & 1.299 & 1.997 & 1.594 & 2.619 \\
12 & 1.284 & 2.000 & 1.633 & 2.664 \\
13 & 1.258 & 1.992 & 1.653 & 2.691 \\
14 & 1.241 & 1.991 & 1.682 & 2.730 \\
15 & 1.226 & 1.991 & 1.714 & 2.767 \\
16 & 1.214 & 1.994 & 1.749 & 2.807 \\
17 & 1.205 & 1.995 & 1.794 & 2.853 \\
18 & 1.196 & 1.993 & 1.845 & 2.901 \\
19 & 1.190 & 1.996 & 1.892 & 2.947 \\
20 & 1.184 & 1.997 & 1.940 & 2.990 \\
21 & 1.177 & 1.996 & 1.982 & 3.027 \\
22 & 1.173 & 1.997 & 2.031 & 3.067 \\
23 & 1.165 & 1.997 & 2.069 & 3.101 \\
24 & 1.159 & 1.998 & 2.111 & 3.135 \\
25 & 1.155 & 2.000 & 2.156 & 3.170 \\
  \bottomrule
\end{tabular}
\end{table}

\subsection{Description of RNN model hyperparameters}

The following experiments investigate deep RNN models parameterized by the
following hyperparameters:
\begin{enumerate}
  \item \texttt{num\_layers} -- the number of memory cell layers
  \item \texttt{rnn\_size} -- the number of hidden units per memory cell (\ie hidden state dimension)
  \item \texttt{wordvec} -- dimension of vector embeddings
  \item \texttt{seq\_length}
  \item \texttt{dropout} -- the dropout probability
\end{enumerate}

\mynote{Does this need to be diagrammed?}
\mynote{Justify wordvec embedding}

Our model first embeds the inputs $\x_t$ into a \texttt{wordvec}-dimensional
vector-space, compressing the dimensionality down from $|V| \approx 140$ to
$\texttt{wordvec}$ dimensions. Next, \texttt{num\_layers} layers of memory
cells followed by batch normalization \citep{ioffe2015batch} and dropout
\citep{hinton2012improving} with dropout probability \texttt{dropout} are
stacked. The outputs $\y^{(\texttt{num\_layers})}_t$ are followed by a
fully-connected layer mapping to $|V| = 108$ units, which are passed through a
softmax to yield a predictive distribution $P(\x_{t+1} | \h_{t-1}, \x_{t})$.
Cross entropy is used as the loss minimized during training.

Models were trained using the Adam normalization \citep{kingma2014adam}
and an initial learning rate of $2 \times 10^{-3}$ decayed by $0.5$ every $5$
epochs. The back-propogation through time gradients were clipped
at $t$\citep{Pascanu2012} and truncated after \texttt{seq\_length} time steps.
We use a mini-batch size of $50$.


\subsection{Comparison of memory cells on music data}

To rapidly compare a large number of memory cell implementations, we leveraged
\texttt{theanets}\footnote{https://github.com/lmjohns3/theanets}: a Python
software library for high-level specification of neural network models.
\Cref{fig:theanets-architecture} shows the results of exploring a range of RNN
memory cell implementation and holding \texttt{num\_layers=1},
\texttt{rnn\_size=130}, \texttt{wordvec=64}, and \texttt{seq\_length=50}
constant. Unlike later models, none of these models utilized dropout or batch
normalization. We used a batch size of $50$ and configured the clockwork RNN
\citep{cho2014learning} with $5$ equal-sized hidden state blocks with update
periods $(1, 2, 4, 8, 16)$.

\begin{figure}[tb]
    \centering
    \input{Chapter4/Figs/theanets-architecture.pgf}
    \caption{LSTM and GRUs yield the lowest training loss. Validation loss
      traces show all architectures exhibit signs of significant overfitting}
    \label{fig:theanets-architecture}
\end{figure}

\Cref{fig:theanets-architecture} shows that while all models achieved similar
validation losses, LSTM and GRUs trained much faster and achieved lower
training loss. Since \citet{zaremba2015empirical} find similar empirical
performance between LSTM and GRUs and \citet{Nayebi2015} observe LSTM
outperforming GRUs in music applications, we choose to use LSTM as the memory
cell for all following experiments.

The increasing validation loss over time in \cref{fig:theanets-architecture}
is a red flag suggesting that overfitting is occuring. This observation motivates
the exporation of dropout regularization in \cref{sec:lstm-dropout}.

\subsection{Optimizing the LSTM architecture}\label{sec:lstm-grid-search}
\nomenclature[z-CPU]{CPU}{Central Processing Unit}
\nomenclature[z-GPU]{GPU}{Graphics Processing Unit}

After settling on LSTM as the memory cell, we conducted remaining experiments
using the \texttt{torch-rnn} Lua software library. Our switch was motivated by
support for GPU training, dropout, and batch normalization.

\subsubsection{Dropout regularization improves validation loss}\label{sec:lstm-dropout}

\mynote{Is it still worth it to do some batch normalization experiments?}

The increasing validation errors in \vref{fig:theanets-architecture} prompted
investigation of regularization techniques. In addition to adding batch
normalization, a technique known to reduce overfitting and accelerate training
\cite{ioffe2015batch}, we also investigated the effects of different levels
of dropout by varying the \texttt{dropout} parameter.

\begin{figure}[tb]
  \centering
  \input{Chapter4/Figs/torch-rnn-dropout.pgf}
  \caption{Dropout acts as a regularizer, resulting in larger training loss
  but better generalization as evidenced by lower validation loss. A setting of
\texttt{dropout=0.3} achieves best results for our model.}
  \label{fig:torch-rnn-dropout}
\end{figure}

The experimental results are shown in \cref{fig:torch-rnn-dropout}. As
expected, dropout acts as a regularizer and reduces validation loss from $0.65$
down to $0.477$ (when \texttt{dropout=0.3}). Training loss has slightly
increased, which is also unexpected as application of dropout during training
introduces additional noise into the model.

\subsubsection{Overall best model}\label{sec:overall-best-model}

We perform a grid search through the following parameter grid:
\begin{itemize}
  \item \texttt{num\_layers} $\in \{1,2,3,4\}$
  \item \texttt{rnn\_size} $\in \{128, 256, 384, 512\}$
  \item \texttt{wordvec} $\in \{16, 32, 64\}$
  \item \texttt{seq\_length} $\in \{64,128,256\}$
  \item \texttt{dropout} $\{0.0, 0.1, 0.2, 0.3, 0.4, 0.5 \}$
\end{itemize}
A full listing of results is provided in \vref{tab:torch-rnn-config-perfs}.

The optimal hyperparameter settings within our grid was found to be
$\texttt{num\_layers}=3$, $\texttt{rnn\_size}=$, $\texttt{wordvec}=32$,
$\texttt{seq\_length}=128$ $\texttt{dropout}=0.3$. Such a model achieves
$0.324$ and $0.477$ cross entropy losses on training and validation corpuses
respectively. \Cref{fig:torch-rnn-best-model-trace} plots the training curve
of this model and shows that training converges after only 30 iterations.

\begin{figure}[tb]
  \centering
  \input{Chapter4/Figs/torch-rnn-best-model-trace.pgf}
  \caption{Training curves for the overall best model. The periodic spikes correspond
  to resetting of the LSTM state at the end of a training epoch.}
  \label{fig:torch-rnn-best-model-trace}
\end{figure}

To confirm local optimality, we perform perturbations about our final
hyperparameter settings in
\crefrange{fig:torch-rnn-network-params}{fig:torch-rnn-input-params-wordvec}.

Consistent with prior work \citep{sutskever2014sequence,kaiser2015neural}, the
\cref{tab:gpu-training} shows that GPU training resulted in training speedups
of more than $800\%$ over CPU training.

\begin{table}[tb]
  \centering
  \caption{Timing results comparing CPU and GPU training of the overall best model (\vref{sec:overall-best-model})}
  \label{tab:gpu-training}
  \begin{tabular}{l c c c}
    \toprule
    \multirow{2}{*}{} & \multicolumn{2}{c}{Single Batch} & 30 Epochs (seconds) \\
    \cmidrule{2-4}
    & mean (sec) & std (sec) & (minutes)\\
    \midrule
    CPU & 4.287 & 0.311 & 256.8\\
    GPU & 0.513 & 0.001 &  28.5\\
    \bottomrule
  \end{tabular}
\end{table}


Sensitivity to network structure: \texttt{num\_layers} and \texttt{rnn\_size}.
\begin{itemize}
    \item Larger \texttt{rnn\_size} leads to higher capacity and lower training loss
        \begin{itemize}
            \item Presents as overfitting on validation, where the lowest capacity
                model \texttt{rnn\_size} appears to be improving in generalization while
                others are flat/increasing
        \end{itemize}
    \item Training curves about the same wrt \texttt{num\_layers}, validation curves have interesting story
        \begin{itemize}
            \item Depth matters: small 64 and 128 hidden unit RNNs saw improvements up to 0.09
            \item Unsurprising, \citet{pascanu2013construct} already observed deep RNNs dominated polyphonic modelling.
            \item Expressivity gained from depth furthers overfitting: 256
                hidden unit RNN has some of the best validation performance at
                depth 1 but is the worst generalizing model for depths 2
                and 3 even though training loss is low
        \end{itemize}
    \item \texttt{rnn\_size=128} found to be best generalizing, optimized at
        \texttt{num\_layers=2}: will continue with these settings
\end{itemize}

Sensitivity to network inputs: \texttt{seq\_length} and \texttt{wordvec}
\begin{itemize}
    \item Training losses are about the same across all \texttt{wordvec}s
    \item Validation losses suggest that increasing \texttt{seq\_length} important for good performance \mynote{investigate further}
    \item \texttt{wordvec=128} overfits for all cases, the other two depend on \texttt{seq\_length} and vary an order of magnitude smaller than the performance gains from increasing \texttt{seq\_length}
\end{itemize}

\section{Results}

As done by \citep{bayer2013fast,Boulanger-Lewandowski2012}, we
quantitatively evaluate our models using cross entropies and
perplexities on a 10\% held-out validation set.

\citet{Allan2005} achieve cross-entropy losses of $2.79-2.80$ on unseen
test-set scores respectively for their ``harmonic skeleton'' sub-task. This task
involves predicting a sequence of one of 81 harmonic symbols, which may be
interpreted as equivalence classes of chords. Even after applying Viterbi
decoding to find globally optimal sequences, their HMM models achieve cross
entropies of $0.84-0.87$ on training-set scores (Table 5.2 in
\citet{Allan2005}).

Our best model achieves cross-entropy losses of $0.323$ on
training data and $0.477$ on held-out test data
(\vref{tab:torch-rnn-config-perfs}), corresponding to a training perplexity of
$1.251$ bits and a test perplexity of $1.391$. This is more than $0.6$ bits
lower than any test perplexity obtained by the $N$-gram models compared in
\vref{tab:baseline-perplexity}. This result is expected and shows that the
greater modelling capacity provided by RNNs is useful for encoded music score
data.

\mynote{Compare \citep{Brien2016}}.

The best models investigated in \citet{Boulanger-Lewandowski2012} achieved
$-5.56$ log likelihood and $33.12\%$ accuracy on the symbolic prediction task.
Note that the log likelihood is significantly lower than the $-0.323$ log
likelihood implied by our model's cross-entropy loss. This is because the two
results are not comparable as they utilize different input encodings with
varying vocabulary sizes.

\mynote{Compare on pitch/pitch class usage, note durations, meter, lengths of compositions}

\section{Other applications}

Scoring things as ``Bach-like'', model for expectation by using the probability.
